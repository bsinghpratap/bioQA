{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b23a6-a674-458b-84da-bc2456887356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "from PubMedQAData import QADataLoader\n",
    "import wandb\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebc702-70e9-4e82-a31a-b32030429e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            finetuning_task='pubmedqa'\n",
    "        )\n",
    "        self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(**batch_)\n",
    "        #pooled = torch.mean(outputs[0], dim=1).to(device)\n",
    "        #logits_ = self.classifier(pooled)\n",
    "        logits_ = outputs[0]\n",
    "        \n",
    "        return logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cca134-3065-4661-8c81-ed66798a8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for collecting all predictions on the input dataset\n",
    "def get_predictions(model_, loader_):\n",
    "    model_.eval()\n",
    "    \n",
    "    #\n",
    "    dict_results = {}\n",
    "    all_preds = []\n",
    "    for batch_idx, batch_ in tqdm(enumerate(loader_)):\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            # unroll features\n",
    "            input_batch = {\n",
    "                'input_ids':batch_['input_ids'],\n",
    "                'attention_mask':batch_['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "            \n",
    "            # update\n",
    "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "            all_preds += preds\n",
    "            ids_ = batch_['ids'].numpy().tolist()\n",
    "            for id_idx, id_ in enumerate(ids_):\n",
    "                dict_results[str(id_)] = {'custom_label': preds[id_idx]}\n",
    "    \n",
    "    # get distribution of predicted labels\n",
    "    count = {}\n",
    "    count['yes'] = (np.array(all_preds) == 0).sum()\n",
    "    count['no'] = (np.array(all_preds) == 1).sum()\n",
    "    count['maybe'] = (np.array(all_preds) == 2).sum()\n",
    "    dist_class = {}\n",
    "    for i in ['yes', 'no', 'maybe']:\n",
    "        dist_class[i] = count[i]/len(all_preds)\n",
    "    \n",
    "    return dict_results, dist_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16cba2-9559-4b62-b67f-d50976bd1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we get the data with artificial label we will need to convert it back to the required format, following class does that\n",
    "\n",
    "class CustomArtiDataloader():\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dict_data: dict,\n",
    "        label2id: dict,\n",
    "        batch_size: int = 16,\n",
    "        debug: bool = False,\n",
    "        debug_size: int = 8,\n",
    "    ):\n",
    "        data = self.to_list(dict_data)\n",
    "        \n",
    "        # define Dataset object\n",
    "        self.dataset = CustomArtiDataset(data)\n",
    "        \n",
    "        # define dataloader object\n",
    "        self.dataloader = Dataloader(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collation_f,            \n",
    "        )\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def to_list(self, data_in):\n",
    "        \n",
    "        data_out = []\n",
    "        for idx_ in range(len(data_in['input_ids'])):\n",
    "            instance = {k_: v_[idx_] for k_, v_ in data_in.items()}\n",
    "            data_out.append(instance)\n",
    "            \n",
    "        return data_out\n",
    "    \n",
    "    def collation_f(self, batch):\n",
    "        \n",
    "        #\n",
    "        input_ids_list = [ex[\"input_ids\"] for ex in batch]\n",
    "        attention_mask_list = [ex[\"attention_mask\"] for ex in batch]\n",
    "        decoder_input_ids_list = [ex[\"decoder_input_ids\"] for ex in batch]\n",
    "        decoder_attention_mask_list = [ex[\"decoder_attention_mask\"] for ex in batch]\n",
    "        decoder_labels_list = [ex[\"decoder_labels\"] for ex in batch]\n",
    "        encoder_label_list = [ex['encoder_labels_artificial'] for ex in batch]\n",
    "\n",
    "        collated_batch = {\n",
    "            \"input_ids\": torch.LongTensor(input_ids_list),\n",
    "            \"attention_mask\": torch.LongTensor(attention_mask_list),\n",
    "            \"encoder_labels\": torch.LongTensor(encoder_label_list),\n",
    "            \"decoder_input_ids\": torch.LongTensor(decoder_input_ids_list),\n",
    "            \"decoder_attention_mask\": torch.LongTensor(decoder_attention_mask_list),\n",
    "            \"decoder_labels\": torch.LongTensor(decoder_labels_list),\n",
    "        }\n",
    "\n",
    "        return collated_batch\n",
    "    \n",
    "class CustomArtiDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, list_data):\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return list_data[idx]\n",
    "\n",
    "#\n",
    "def inspect_dataloader(loaders):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    #\n",
    "    print(f\"\\nSize of the training set is {len(loaders.dataset_train)}\")\n",
    "    print(f\"Size of the validation set is {len(loaders.dataset_validation)}\")\n",
    "    print(f\"Size of the test set is {len(loaders.dataset_test)}\")\n",
    "    \n",
    "    #\n",
    "    check_first = loaders.dataset_validation[0]['input_ids'] == loaders.dataset_test[0]['input_ids']\n",
    "    check_last = loaders.dataset_validation[-1]['input_ids'] == loaders.dataset_test[-1]['input_ids']\n",
    "    print(f\"\\nFirst example in test and validation set is same: {check_first}\")\n",
    "    print(f\"Last example in test and validation set is same: {check_last}\")\n",
    "    \n",
    "    # check if train example exists in test or validation set\n",
    "    with open('test_set.json', 'r') as f:\n",
    "        test_ = json.load(f)\n",
    "    with open('dev_set.json', 'r') as f:\n",
    "        dev_ = json.load(f)\n",
    "    check_pool = list(test_.keys()) + list(dev_.keys())\n",
    "    \n",
    "    \n",
    "    # check distribution of all classes in train, test and valid\n",
    "    id2label = {0: 'yes', 1: 'no', 2: 'maybe'}\n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_train))):\n",
    "        label_i = loaders.dataset_train[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in training set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_train)}\")\n",
    "        \n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_validation))):\n",
    "        label_i = loaders.dataset_validation[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in validation set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_validation)}\")\n",
    "    \n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_test))):\n",
    "        label_i = loaders.dataset_test[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in test set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_test)}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\nChecking if training examples exists in test.dev set...\")\n",
    "    for idx in range(len(loaders.dataset_train)):\n",
    "        train_i = loaders.dataset_train[idx]\n",
    "        id_ = train_i['id'][0]\n",
    "        assert id_ not in check_pool, \"Training exampl exists in test/dev set, check dataloader\"\n",
    "    \n",
    "    #\n",
    "    print(\"\\nPrinting three randomly sampled examples...\")\n",
    "    random_samples = np.random.randint(0, len(loaders.dataset_train), size=3)\n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = loaders.dataset_train[sample_]\n",
    "        tokenizer = loaders.source_tokenizer\n",
    "        id2label = loaders.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('Gold label is as follows:')\n",
    "        print(id2label[tokenized_sample['gold_label'][0]])\n",
    "    \"\"\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6789a-c637-4347-bddf-0a8ed73a8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2:\n",
    "# Step 1: get dataloader for unlabled and artifial dataset\n",
    "# Step 2: instantiate biomed-roberta model and load previously trained model\n",
    "# Step 3: use loaded model to predict artificial labels\n",
    "# Step 4: convert the predictions into dataloader\n",
    "# Step 5: train BioMedRoberta on artificial data\n",
    "# Step 6: save the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7cd01-f0ee-4812-951a-ce18dc4d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 10,\n",
    "    'learning_rate': 6.2e-6,\n",
    "    'epochs': 100,\n",
    "    'eval_every_steps': 300,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 768,\n",
    "    'output_dir': r'./local_biomed_roberta_base',\n",
    "}\n",
    "label2id = {\n",
    "    'yes': 0,\n",
    "    'no': 1,\n",
    "    'maybe': 2,\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "#\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'allenai/biomed_roberta_base',\n",
    "        'tokenizer': 'allenai/biomed_roberta_base',\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'RoBERTa-large-PM-M3-hf',\n",
    "        'tokenizer': 'roberta-large',\n",
    "    },\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983248f-7bd3-4629-885a-527bd6025e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Dataloader\n",
    "\n",
    "#\n",
    "data_all = QADataLoader(\n",
    "    datasets_name=None,#'pubmed_qa',\n",
    "    datasets_config=None,#'pqa_artificial',\n",
    "    label2id=label2id,\n",
    "    tokenizer_name=model_dict[0]['tokenizer'],\n",
    "    max_sequence_length=args['max_sequence_length'],\n",
    "    batch_size=args['batch_size'],\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b978e-3176-4d88-bce4-bba1224d667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect_dataloader(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327259f4-6fef-4b95-933d-755bd165ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model\n",
    "\n",
    "#\n",
    "model_name = model_dict[0]['model'].split('/')[-1]\n",
    "model = QAModel(\n",
    "    model_name= model_dict[0]['model'],\n",
    "    num_classes=data_all.num_classes,\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(args['output_dir'],  model_name+'_phase1_.pt')))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18ad78-a56d-4e28-ae92-363539823d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Predict (get artificial labels)\n",
    "predictions, dist_pred = get_predictions(model, data_all.dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5cd037-0ecd-469f-a686-c6721e9faf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fc358-3c08-44b7-9514-b20e7ca50fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'yes',\n",
    "    1: 'no',\n",
    "    2: 'maybe',\n",
    "}\n",
    "\n",
    "model_labeled_data = {}\n",
    "\n",
    "with open('ori_pqaa.json', 'r') as f:\n",
    "    a_ = json.load(f)\n",
    "with open('ori_pqau.json', 'r') as f:\n",
    "    u_ = json.load(f)\n",
    "\n",
    "for data_ in [a_, u_]:\n",
    "    for id_idx, id_ in enumerate(data_):\n",
    "        model_labeled_data[id_] = data_[id_]\n",
    "        if id_ in predictions:\n",
    "            model_labeled_data[id_]['custom_label'] = id2label[predictions[id_]['custom_label']]\n",
    "        else:\n",
    "            model_labeled_data[id_]['custom_label'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf568f0-f67b-4b84-901a-c347e3bf9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labeled_data['25429730']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5603c5d-60c1-4005-bbac-95f6961da00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_labeled_data.json', 'w') as f:\n",
    "    json.dump(model_labeled_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8ac8d-24a7-4134-84d9-78d636f4254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_labeled_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7537a90-e331-4e1c-92ee-9ff60e6d9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in data:\n",
    "    print(id_)\n",
    "    print(type(data[id_]))\n",
    "    print(data[id_].keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71273d-9df3-45cd-a462-afdc3c06375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in data:\n",
    "    if data[i_]['custom_label'] != '':\n",
    "        count[data[i_]['custom_label']] += 1\n",
    "\n",
    "dist_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in count:\n",
    "    dist_[i_] = count[i_] / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb41be-cb17-4074-8c40-0c20e28da03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dc028-abd3-4702-8c42-4883edecbc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ori_pqaa_1st_attempt.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "count = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in data:\n",
    "    if data[i_]['custom_label'] != '':\n",
    "        count[data[i_]['custom_label']] += 1\n",
    "\n",
    "dist_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in count:\n",
    "    dist_[i_] = count[i_] / len(data)\n",
    "\n",
    "print(dist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318010be-887f-4a7b-b9b7-4d6c81967530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
