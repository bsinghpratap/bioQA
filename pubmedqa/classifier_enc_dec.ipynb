{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aece49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='1'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] ='4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8235d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705aeaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CS5520_1/miniconda3/envs/nlp_/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    ")\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dff83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2dee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(split, fold=1):\n",
    "    if split == 'train':\n",
    "        train_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/train_set.json' % fold, \n",
    "                                    'r'))\n",
    "        dev_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/dev_set.json' % fold, \n",
    "                                  'r'))\n",
    "        final_json = {**train_json, **dev_json}\n",
    "    else:\n",
    "        test_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/test_set.json', 'r'))\n",
    "        final_json = test_json\n",
    "    list_data = []\n",
    "    for key_, val_ in final_json.items():\n",
    "        tmp_ = {'sentence1': val_['QUESTION'], \n",
    "                'sentence2': ' '.join(val_['CONTEXTS']), \n",
    "                'gold_label': val_['final_decision']}\n",
    "        list_data.append(tmp_)\n",
    "    return list_data\n",
    "\n",
    "def read_data_(dict_data_):\n",
    "    \n",
    "    list_data = []\n",
    "    for idx in range(len(dict_data_['question'])):\n",
    "        instance = {\n",
    "            'sentence1': dict_data_['question'][idx],\n",
    "            'sentence2': ''.join(dict_data_['context'][idx]['contexts']),\n",
    "            'gold_label': dict_data_['final_decision'][idx]\n",
    "        }\n",
    "        list_data.append(instance)\n",
    "    \n",
    "    return list_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629018c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_wts(dict_cnt, alpha=15):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = np.log(alpha * tot_cnt/dict_cnt[each_cat])\n",
    "    return wt_\n",
    "\n",
    "def get_class_dist(dict_cnt):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = dict_cnt[each_cat]/tot_cnt\n",
    "    return wt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3588c44b-c86e-4d82-9cd7-9317a473b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/CS5520_1/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 391.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pubid', 'question', 'context', 'long_answer', 'final_decision'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pubmedqa = datasets.load_dataset('pubmed_qa', 'pqa_artificial')\n",
    "pubmedqa_train, pubmedqa_test = train_test_split(pubmedqa['train'])\n",
    "\n",
    "pubmedqa_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f827a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_data = {}\n",
    "#dict_data['train'] = read_data(split='train', fold=1)\n",
    "#dict_data['test'] = read_data(split='test')\n",
    "dict_data['train'] = read_data_(pubmedqa_train)\n",
    "dict_data['test'] = read_data_(pubmedqa_test)\n",
    "\n",
    "label2id = {'yes':0, 'no': 1, 'maybe': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6815ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'The colour of pain: can patients use colour to describe osteoarthritis pain?',\n",
       " 'sentence2': \"The aim of the present study was to explore patients' views on the acceptability and feasibility of using colour to describe osteoarthritis (OA) pain, and whether colour could be used to communicate pain to healthcare professionals.Six group interviews were conducted with 17 patients with knee OA. Discussion topics included first impressions about using colour to describe pain, whether participants could associate their pain with colour, how colours related to changes to intensity and different pain qualities, and whether they could envisage using colour to describe pain to healthcare professionals.The group interviews indicated that, although the idea of using colour was generally acceptable, it did not suit all participants as a way of describing their pain. The majority of participants chose red to describe high-intensity pain; the reasons given were because red symbolized inflammation, fire, anger and the stop signal in a traffic light system. Colours used to describe the absence of pain were chosen because of their association with positive emotional feelings, such as purity, calmness and happiness. A range of colours was chosen to represent changes in pain intensity. Aching pain was consistently identified as being associated with colours such as grey or black, whereas sharp pain was described using a wider selection of colours. The majority of participants thought that they would be able to use colour to describe their pain to healthcare professionals, although issues around the interpretability and standardization of colour were raised.\",\n",
       " 'gold_label': 'yes'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c290a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Train\n",
      "====================\n",
      "Train:  Counter({'yes': 416, 'no': 257, 'maybe': 77})\n",
      "Train:  94.79066666666667\n",
      "Train:  1353.1506666666667\n",
      "\n",
      "\n",
      "====================\n",
      "Test\n",
      "====================\n",
      "Test:  Counter({'yes': 136, 'no': 81, 'maybe': 33})\n",
      "Test:  92.412\n",
      "Test:  1296.172\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*10)\n",
    "print('Train')\n",
    "print(\"==\"*10)\n",
    "class_counts = Counter([x['gold_label'] for x in dict_data['train']])\n",
    "print(\"Train: \", Counter([x['gold_label'] for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence1'].__len__() for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence2'].__len__() for x in dict_data['train']]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"==\"*10)\n",
    "print(\"Test\")\n",
    "print(\"==\"*10)\n",
    "print(\"Test: \", Counter([x['gold_label'] for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence1'].__len__() for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence2'].__len__() for x in dict_data['test']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8aef64-9d0c-4553-9c54-6834a7df1d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b3c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 1.6880002349372025, 'no': 2.169609410303246, 'maybe': 3.374880073344782}\n",
      "{'yes': 0.5546666666666666, 'no': 0.3426666666666667, 'maybe': 0.10266666666666667}\n"
     ]
    }
   ],
   "source": [
    "#class_wts = get_class_wts(dict_cnt={'yes': 276, 'no': 169, 'maybe': 55}, \n",
    "#                          alpha=3)\n",
    "\n",
    "class_wts = get_class_wts(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }, \n",
    "    alpha=3\n",
    ")\n",
    "print(class_wts)\n",
    "\n",
    "class_dist = get_class_dist(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }\n",
    ")\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4cda53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        #\n",
    "        model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.encoder = model.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=batch_['input_ids'],\n",
    "            attention_mask=batch_['attention_mask'],\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        # extract encoder output\n",
    "        encodings = outputs['encoder_last_hidden_state']\n",
    "        pooled = torch.mean(encodings, dim=1)\n",
    "        logits_enc = self.classifier(pooled)\n",
    "        \n",
    "        #\n",
    "        logits_dec = outputs['last_hidden_state']\n",
    "        \n",
    "        return logits_enc, logits_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17afa8a6-08e4-4460-be7c-7d1f0922ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilliary functions\n",
    "\n",
    "def inspect_dataloader(dataloader_, ):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    random_samples = np.random.randint(0, len(dataloader_.dataset_train), size=3)\n",
    "    \n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = dataloader_.dataset_train[sample_]\n",
    "        tokenizer = dataloader_.tokenizer\n",
    "        id2label = dataloader_.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('Gold label is as follows:')\n",
    "        print(id2label[tokenized_sample['label'].item()])        \n",
    "    \n",
    "    return\n",
    "\n",
    "def get_grouped_parameters(\n",
    "    model_in, \n",
    "    no_decay_layers, \n",
    "    weight_decay\n",
    "):\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model_in.named_parameters() if not any(nd in n for nd in no_decay_layers)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model_in.named_parameters() if any(nd in n for nd in no_decay_layers)], \n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "def evaluate(model, data_loader, objective_f):\n",
    "    model.eval()\n",
    "    dict_result = {'actual':[],\n",
    "                   'preds':[]}\n",
    "    \n",
    "    #print('\\nStarting model evaluation:')\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            \n",
    "            # unroll features\n",
    "            dict_result['actual'] += batch['encoder_labels'].numpy().tolist()\n",
    "            input_batch = {\n",
    "                'input_ids':batch['input_ids'],\n",
    "                'attention_mask':batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            logits, _ = model(input_batch)\n",
    "            \n",
    "            # calculate loss\n",
    "            eval_loss += objective_f(\n",
    "                logits, \n",
    "                batch['encoder_labels'].to(device)\n",
    "            ).item()\n",
    "            \n",
    "            # update\n",
    "            dict_result['preds'] += np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "    \n",
    "    # update\n",
    "    dict_result['actual'] = [x for x in dict_result['actual']]\n",
    "    dict_result['loss'] = eval_loss / (batch_idx + 1)\n",
    "    \n",
    "    return dict_result\n",
    "\n",
    "def get_performance(\n",
    "    actual_, \n",
    "    preds_,\n",
    "    dict_mapping\n",
    "):\n",
    "    results = {}\n",
    "    \n",
    "    # accuracy, precision, recall, f1\n",
    "    results['metrics'] = classification_report(\n",
    "        actual_, \n",
    "        preds_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    \n",
    "    # confusion matrix\n",
    "    results['confusion_matrix'] = pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            actual_, \n",
    "            preds_\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # counter\n",
    "    results['actual_counter'] = Counter(actual_)\n",
    "    results['prediction_counter'] = Counter(preds_)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaab0f-98e9-4536-b861-a91d27f73841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec39a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'roberta-base'\n",
    "#tokenizer_name = 'roberta-base'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 15,\n",
    "    'learning_rate': 6.5e-6,\n",
    "    'epochs': 400,\n",
    "    'eval_every_steps': 150,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 16,\n",
    "    'wt_classification': 0.5,\n",
    "    'wt_generation': 0.5,\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c052db2e-c4f9-45ca-96aa-302b9a4f987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from PubMedQAData_EncDec import QADataLoader\n",
    "labe2id = {'yes': 0, 'no': 1, 'maybe': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c68844-0ef8-4f4f-b6ca-f51384ec9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    1: {\n",
    "        'model': 'GanjinZero/biobart-base',\n",
    "        'tokenizer': 'GanjinZero/biobart-base',\n",
    "    },\n",
    "    2: {\n",
    "        'model': r'./results',\n",
    "        'tokenizer': 'GanjinZero/biobart-base',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b51771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training of model: GanjinZero/biobart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvijetakd\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nonraidv/home/CS5520_1/vijeta_trial/bioQA/pubmedqa/wandb/run-20220430_100611-sxu6r2bt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/sxu6r2bt\" target=\"_blank\">gentle-resonance-73</a></strong> to <a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/CS5520_1/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 359.29it/s]\n",
      " 16%|█████████████▊                                                                        | 64/400 [21:47<1:57:29, 20.98s/it]wandb: Network error (ReadTimeout), entering retry loop.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 400/400 [2:16:19<00:00, 20.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====\n",
      "Results for model\t : GanjinZero/biobart-base\n",
      "=====\n",
      "Precision \t\t = 0.370567\n",
      "Recall \t\t\t = 0.386032\n",
      "f1-score \t\t = 0.365651\n",
      "Accuracy \t\t = 0.576000\n",
      "=====\n",
      "\n",
      "Starting training of model: ./results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:sxu6r2bt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>evaluation_accuracy</td><td>▂▂▂▂▂▂▁▃▅▄▆▃▇▄█▅▂▂▆▆▇▆▅▆▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>evaluation_f1</td><td>▁▁▁▁▁▁▁▄▅██▇▆▇█▇▁▁▅▆▆▆▅▆▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>evaluation_loss</td><td>▂▂▁▁▁▁▁▂▃▄▄▅▆▅▆▆▁▁▄▅▆▆▆▆▇▇▇▇▇███████████</td></tr><tr><td>evaluation_precision</td><td>▁▁▁▁▁▁▅▇▇▇▇▇█▇█▇▁▁▇▇█▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇█▇▇</td></tr><tr><td>evaluation_precision_maybe</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>evaluation_precision_no</td><td>▁▁▁▁▁▁▅█▇▇▇▇█▇█▇▁▁▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇</td></tr><tr><td>evaluation_precision_yes</td><td>▁▁▁▁▁▁▁▃▄▇█▆▆▇▇▇▁▁▅▆▆▆▅▆▅▄▅▅▅▅▅▅▅▅▅▅▅▅▆▆</td></tr><tr><td>evaluation_recall</td><td>▁▁▁▁▁▁▁▄▅▇█▆▇▆█▇▁▁▅▆▆▆▅▆▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>▇██▆█▆▆▅▃▂▂▂▃▁▂▁█▇▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>399</td></tr><tr><td>evaluation_accuracy</td><td>0.592</td></tr><tr><td>evaluation_f1</td><td>0.34949</td></tr><tr><td>evaluation_loss</td><td>1.46848</td></tr><tr><td>evaluation_precision</td><td>0.38644</td></tr><tr><td>evaluation_precision_maybe</td><td>0.0</td></tr><tr><td>evaluation_precision_no</td><td>0.55556</td></tr><tr><td>evaluation_precision_yes</td><td>0.60377</td></tr><tr><td>evaluation_recall</td><td>0.3847</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.06804</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">gentle-resonance-73</strong>: <a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/sxu6r2bt\" target=\"_blank\">https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/sxu6r2bt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_100611-sxu6r2bt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:sxu6r2bt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nonraidv/home/CS5520_1/vijeta_trial/bioQA/pubmedqa/wandb/run-20220430_122248-2cwcvuvg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/2cwcvuvg\" target=\"_blank\">snowy-wildflower-74</a></strong> to <a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/CS5520_1/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 338.93it/s]\n",
      "  1%|█                                                                                      | 5/400 [01:40<2:13:49, 20.33s/it]wandb: Network error (ReadTimeout), entering retry loop.\n",
      " 86%|████████████████████████████████████████████████████████████████████████▉            | 343/400 [1:57:21<19:31, 20.56s/it]wandb: Network error (ReadTimeout), entering retry loop.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 400/400 [2:16:52<00:00, 20.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====\n",
      "Results for model\t : ./results\n",
      "=====\n",
      "Precision \t\t = 0.496070\n",
      "Recall \t\t\t = 0.411824\n",
      "f1-score \t\t = 0.420884\n",
      "Accuracy \t\t = 0.584000\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for model_idx in model_dict:\n",
    "    print('\\nStarting training of model: %s'%(model_dict[model_idx]['model']))\n",
    "    \n",
    "    #\n",
    "    args['model'] = model_dict[model_idx]['model']\n",
    "    wandb.init(\n",
    "        project='Bio-Med-Clinical QA', \n",
    "        config=args\n",
    "    )\n",
    "    \n",
    "    # get dataloaders for training and testing\n",
    "    dataloaders = QADataLoader(\n",
    "        datasets_name='pubmed_qa',\n",
    "        datasets_config='pqa_artificial',\n",
    "        label2id=label2id,\n",
    "        tokenizer_name=model_dict[model_idx]['tokenizer'],\n",
    "        max_sequence_length=args['max_sequence_length'],\n",
    "        batch_size=args['batch_size'],\n",
    "        debug=True\n",
    "    )\n",
    "    inspect_dataloader(dataloaders)\n",
    "    break\n",
    "\n",
    "    #\n",
    "    train_loader = dataloaders.dataloader_train\n",
    "    val_loader = dataloaders.dataloader_validation\n",
    "    test_loader = dataloaders.dataloader_test\n",
    "    \n",
    "    # set total steps and warmp-up steps for sheduler\n",
    "    args['t_total'] = len(train_loader) // args['gradient_accumulation_steps'] * args['epochs']\n",
    "    args['warmup_steps'] = int(0.10*args['t_total'])\n",
    "\n",
    "    # define model\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dict[model_idx]['model'], \n",
    "        config=config,\n",
    "    )\n",
    "    \"\"\"\n",
    "    #\n",
    "    model = QAModel(\n",
    "        model_name=model_dict[model_idx]['model'],\n",
    "        num_classes=dataloaders.num_classes,\n",
    "    )\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier.weight' in name:\n",
    "            torch.nn.init.zeros_(param.data)\n",
    "        elif 'classifier.bias' in name:\n",
    "            param.data = torch.tensor([class_dist['yes'], class_dist['no'], class_dist['maybe']]).float()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        get_grouped_parameters(model, no_decay, args['weight_decay']), \n",
    "        lr=args['learning_rate'], \n",
    "        eps=args['adam_epsilon']\n",
    "    )\n",
    "\n",
    "    # scheduler for lr\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=args['warmup_steps'],\n",
    "        num_training_steps=args['t_total']\n",
    "    )\n",
    "\n",
    "    # objective function\n",
    "    loss_fct = CrossEntropyLoss(weight=torch.tensor(list(class_wts.values())).to(device))\n",
    "    loss_fct_dec = CrossEntropyLoss()\n",
    "    \n",
    "    # train\n",
    "    best_model = None\n",
    "    best_f1_eval = -1\n",
    "    best_test_results = None\n",
    "    best_val_results = None\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for each_epoch in tqdm(range(args['epochs'])):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            global_step += 1\n",
    "\n",
    "            # clean gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # unroll inputs and sent to device\n",
    "            input_batch = {\n",
    "                'input_ids': batch['input_ids'],\n",
    "                'attention_mask': batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "\n",
    "            # forward pass\n",
    "            logits, logits_dec = model(input_batch)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fct(\n",
    "                logits, \n",
    "                batch['encoder_labels'].to(device)\n",
    "            )\n",
    "            loss_dec = loss_fct_dec(\n",
    "                logits_dec,\n",
    "                batch['decoder_labels'].to(device)\n",
    "            )\n",
    "            \n",
    "            # log info to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"classification_loss\": loss,\n",
    "                    \"generation_loss\": loss_dec,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"epoch\": each_epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "            \n",
    "\n",
    "            # backpropagation\n",
    "            loss = (wt_classification * loss) + (wt_generation * loss_dec)\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters and lr\n",
    "            optimizer.step()\n",
    "            scheduler.step()  \n",
    "\n",
    "            if global_step%args['eval_every_steps'] == 0:\n",
    "                # evaluate model\n",
    "                val_predictions = evaluate(\n",
    "                    model=model, \n",
    "                    data_loader=val_loader,\n",
    "                    objective_f=loss_fct,\n",
    "                )\n",
    "                val_results = get_performance(\n",
    "                    actual_=val_predictions['actual'], \n",
    "                    preds_=val_predictions['preds'], \n",
    "                    dict_mapping=label2id\n",
    "                )\n",
    "\n",
    "                # log info to wandb\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"evaluation_precision\": val_results['metrics']['macro avg']['precision'],\n",
    "                        \"evaluation_recall\": val_results['metrics']['macro avg']['recall'],\n",
    "                        \"evaluation_f1\": val_results['metrics']['macro avg']['f1-score'],\n",
    "                        \"evaluation_accuracy\": val_results['metrics']['accuracy'],\n",
    "                        \"evaluation_loss\": val_predictions['loss'],\n",
    "                        \"epoch\": each_epoch,\n",
    "\n",
    "                        \"evaluation_precision_yes\": val_results['metrics']['0']['precision'],\n",
    "                        \"evaluation_precision_no\": val_results['metrics']['1']['precision'],\n",
    "                        \"evaluation_precision_maybe\": val_results['metrics']['2']['precision'],\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "\n",
    "                # update best model\n",
    "                if best_f1_eval < val_results['metrics']['weighted avg']['f1-score']:\n",
    "                    best_model = deepcopy(model).to(device)\n",
    "                    best_val_results = deepcopy(val_results)\n",
    "                    best_f1_eval = val_results['metrics']['weighted avg']['f1-score']\n",
    "\n",
    "    \n",
    "    # test the model based on best_model\n",
    "    test_predictions = evaluate(\n",
    "        model=best_model, \n",
    "        data_loader=test_loader,\n",
    "        objective_f=loss_fct,\n",
    "    )\n",
    "    best_test_results = get_performance(\n",
    "        actual_=test_predictions['actual'], \n",
    "        preds_=test_predictions['preds'], \n",
    "        dict_mapping=label2id\n",
    "    )\n",
    "    \n",
    "    # save the results and the model\n",
    "    model_dict[model_idx]['results'] = {\n",
    "        'validation_results': deepcopy(best_val_results),\n",
    "        'test_results': deepcopy(best_test_results),\n",
    "        'trained_model': deepcopy(best_model),\n",
    "    }\n",
    "    \n",
    "    #\n",
    "    print('\\n')\n",
    "    print('='*5)\n",
    "    print('Results for model\\t : %s'%model_dict[model_idx]['model'])\n",
    "    print('='*5)\n",
    "    print('Precision \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['precision'])\n",
    "    print('Recall \\t\\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['recall'])\n",
    "    print('f1-score \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['f1-score'])\n",
    "    print('Accuracy \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['accuracy'])\n",
    "    print('='*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd6866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24727afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff68daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27e447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5036be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0ea8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdafb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a47ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da8fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad64f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618c29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59ebe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42d326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786379d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0d164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7704c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
