{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aece49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='1'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] ='4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8235d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705aeaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CS5520_1/miniconda3/envs/nlp_/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    ")\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dff83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2dee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(split, fold=1):\n",
    "    if split == 'train':\n",
    "        train_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/train_set.json' % fold, \n",
    "                                    'r'))\n",
    "        dev_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/dev_set.json' % fold, \n",
    "                                  'r'))\n",
    "        final_json = {**train_json, **dev_json}\n",
    "    else:\n",
    "        test_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/test_set.json', 'r'))\n",
    "        final_json = test_json\n",
    "    list_data = []\n",
    "    for key_, val_ in final_json.items():\n",
    "        tmp_ = {'sentence1': val_['QUESTION'], \n",
    "                'sentence2': ' '.join(val_['CONTEXTS']), \n",
    "                'gold_label': val_['final_decision']}\n",
    "        list_data.append(tmp_)\n",
    "    return list_data\n",
    "\n",
    "def read_data_(dict_data_):\n",
    "    \n",
    "    list_data = []\n",
    "    for idx in range(len(dict_data_['question'])):\n",
    "        instance = {\n",
    "            'sentence1': dict_data_['question'][idx],\n",
    "            'sentence2': ''.join(dict_data_['context'][idx]['contexts']),\n",
    "            'gold_label': dict_data_['final_decision'][idx]\n",
    "        }\n",
    "        list_data.append(instance)\n",
    "    \n",
    "    return list_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629018c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_wts(dict_cnt, alpha=15):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = np.log(alpha * tot_cnt/dict_cnt[each_cat])\n",
    "    return wt_\n",
    "\n",
    "def get_class_dist(dict_cnt):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = dict_cnt[each_cat]/tot_cnt\n",
    "    return wt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3588c44b-c86e-4d82-9cd7-9317a473b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/CS5520_1/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 374.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pubid', 'question', 'context', 'long_answer', 'final_decision'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pubmedqa = datasets.load_dataset('pubmed_qa', 'pqa_labeled')\n",
    "pubmedqa_train, pubmedqa_test = train_test_split(pubmedqa['train'])\n",
    "\n",
    "pubmedqa_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f827a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_data = {}\n",
    "#dict_data['train'] = read_data(split='train', fold=1)\n",
    "#dict_data['test'] = read_data(split='test')\n",
    "dict_data['train'] = read_data_(pubmedqa_train)\n",
    "dict_data['test'] = read_data_(pubmedqa_test)\n",
    "\n",
    "label2id = {'yes':0, 'no': 1, 'maybe': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6815ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Treatment of contralateral hydrocele in neonatal testicular torsion: Is less more?',\n",
       " 'sentence2': 'Treatment of neonatal testicular torsion has two objectives: salvage of the involved testicle (which is rarely achieved) and preservation of the contralateral gonad. The second goal universally involves contralateral testicular scrotal fixation to prevent the future occurrence of contralateral torsion. However, there is controversy with regards to management of a synchronous contralateral hydrocele. It has been our policy not to address the contralateral hydrocele through an inguinal incision to minimize potential injury to the spermatic cord. Our objective in this study was to determine whether the decision to manage a contralateral hydrocele in cases of neonatal testicular torsion solely through a scrotal approach is safe and effective.We reviewed all cases of neonatal testicular torsion occurring at our institution between the years 1999 and 2006. Age at presentation, physical examination, ultrasonographic and intraoperative findings were recorded. Patients were followed after initial surgical intervention to determine the likelihood of developing a subsequent hydrocele or hernia.Thirty-seven patients were identified as presenting with neonatal torsion. Age of presentation averaged 3.5 days (range 1-14 days). Left-sided pathology was seen more commonly than the right, with a 25:12 distribution. All torsed testicles were nonviable. Twenty-two patients were noted to have a contralateral hydrocele at presentation. All hydroceles were opened through a scrotal approach at the time of contralateral scrotal fixation. No patient underwent an inguinal exploration to examine for a patent process vaginalis. None of the patients who presented with a hydrocele have developed a clinical hydrocele or hernia after an average 7.5 years (range 4.3-11.2) follow-up.',\n",
       " 'gold_label': 'maybe'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c290a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Train\n",
      "====================\n",
      "Train:  Counter({'yes': 421, 'no': 252, 'maybe': 77})\n",
      "Train:  94.15333333333334\n",
      "Train:  1342.3773333333334\n",
      "\n",
      "\n",
      "====================\n",
      "Test\n",
      "====================\n",
      "Test:  Counter({'yes': 131, 'no': 86, 'maybe': 33})\n",
      "Test:  94.324\n",
      "Test:  1328.492\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*10)\n",
    "print('Train')\n",
    "print(\"==\"*10)\n",
    "class_counts = Counter([x['gold_label'] for x in dict_data['train']])\n",
    "print(\"Train: \", Counter([x['gold_label'] for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence1'].__len__() for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence2'].__len__() for x in dict_data['train']]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"==\"*10)\n",
    "print(\"Test\")\n",
    "print(\"==\"*10)\n",
    "print(\"Test: \", Counter([x['gold_label'] for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence1'].__len__() for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence2'].__len__() for x in dict_data['test']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8aef64-9d0c-4553-9c54-6834a7df1d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b3c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 1.6760526615160842, 'no': 2.1892564076870427, 'maybe': 3.374880073344782}\n",
      "{'yes': 0.5613333333333334, 'no': 0.336, 'maybe': 0.10266666666666667}\n"
     ]
    }
   ],
   "source": [
    "#class_wts = get_class_wts(dict_cnt={'yes': 276, 'no': 169, 'maybe': 55}, \n",
    "#                          alpha=3)\n",
    "\n",
    "class_wts = get_class_wts(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }, \n",
    "    alpha=3\n",
    ")\n",
    "print(class_wts)\n",
    "\n",
    "class_dist = get_class_dist(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }\n",
    ")\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4cda53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        #\n",
    "        model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.encoder = model.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=batch_['input_ids'],\n",
    "            attention_mask=batch_['attention_mask'],\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        # extract encoder output\n",
    "        encodings = outputs['encoder_last_hidden_state']\n",
    "        pooled = torch.mean(encodings, dim=1)\n",
    "        logits_ = self.classifier(pooled)\n",
    "        \n",
    "        return logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17afa8a6-08e4-4460-be7c-7d1f0922ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilliary functions\n",
    "\n",
    "def inspect_dataloader(dataloader_, ):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    random_samples = np.random.randint(0, len(dataloader_.dataset_train), size=3)\n",
    "    \n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = dataloader_.dataset_train[sample_]\n",
    "        tokenizer = dataloader_.tokenizer\n",
    "        id2label = dataloader_.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('Gold label is as follows:')\n",
    "        print(id2label[tokenized_sample['label'].item()])        \n",
    "    \n",
    "    return\n",
    "\n",
    "def get_grouped_parameters(\n",
    "    model_in, \n",
    "    no_decay_layers, \n",
    "    weight_decay\n",
    "):\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model_in.named_parameters() if not any(nd in n for nd in no_decay_layers)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model_in.named_parameters() if any(nd in n for nd in no_decay_layers)], \n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "def evaluate(model, data_loader, objective_f):\n",
    "    model.eval()\n",
    "    dict_result = {'actual':[],\n",
    "                   'preds':[]}\n",
    "    \n",
    "    #print('\\nStarting model evaluation:')\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            \n",
    "            # unroll features\n",
    "            dict_result['actual'] += batch['encoder_labels'].numpy().tolist()\n",
    "            input_batch = {\n",
    "                'input_ids':batch['input_ids'],\n",
    "                'attention_mask':batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "            \n",
    "            # calculate loss\n",
    "            eval_loss += objective_f(\n",
    "                logits, \n",
    "                batch['encoder_labels'].to(device)\n",
    "            ).item()\n",
    "            \n",
    "            # update\n",
    "            dict_result['preds'] += np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "    \n",
    "    # update\n",
    "    dict_result['actual'] = [x for x in dict_result['actual']]\n",
    "    dict_result['loss'] = eval_loss / (batch_idx + 1)\n",
    "    \n",
    "    return dict_result\n",
    "\n",
    "def get_performance(\n",
    "    actual_, \n",
    "    preds_,\n",
    "    dict_mapping\n",
    "):\n",
    "    results = {}\n",
    "    \n",
    "    # accuracy, precision, recall, f1\n",
    "    results['metrics'] = classification_report(\n",
    "        actual_, \n",
    "        preds_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    \n",
    "    # confusion matrix\n",
    "    results['confusion_matrix'] = pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            actual_, \n",
    "            preds_\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # counter\n",
    "    results['actual_counter'] = Counter(actual_)\n",
    "    results['prediction_counter'] = Counter(preds_)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaab0f-98e9-4536-b861-a91d27f73841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec39a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'roberta-base'\n",
    "#tokenizer_name = 'roberta-base'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 15,\n",
    "    'learning_rate': 6.5e-6,\n",
    "    'epochs': 400,\n",
    "    'eval_every_steps': 150,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 16,\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c052db2e-c4f9-45ca-96aa-302b9a4f987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from PubMedQAData_EncDec import QADataLoader\n",
    "labe2id = {'yes': 0, 'no': 1, 'maybe': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c68844-0ef8-4f4f-b6ca-f51384ec9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    1: {\n",
    "        'model': 'GanjinZero/biobart-base',\n",
    "        'tokenizer': 'GanjinZero/biobart-base',\n",
    "    },\n",
    "    2: {\n",
    "        'model': r'./results',\n",
    "        'tokenizer': 'GanjinZero/biobart-base',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b51771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training of model: GanjinZero/biobart-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvijetakd\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nonraidv/home/CS5520_1/vijeta_trial/bioQA/pubmedqa/wandb/run-20220430_012227-3rbhuy90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/3rbhuy90\" target=\"_blank\">exalted-glade-71</a></strong> to <a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/CS5520_1/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 361.52it/s]\n",
      " 55%|█████████████████████████████████████████████▍                                     | 219/400 [1:15:05<1:01:38, 20.43s/it]wandb: Network error (ReadTimeout), entering retry loop.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 400/400 [2:17:10<00:00, 20.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====\n",
      "Results for model\t : GanjinZero/biobart-base\n",
      "=====\n",
      "Precision \t\t = 0.470771\n",
      "Recall \t\t\t = 0.406543\n",
      "f1-score \t\t = 0.399918\n",
      "Accuracy \t\t = 0.540000\n",
      "=====\n",
      "\n",
      "Starting training of model: ./results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3rbhuy90) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>evaluation_accuracy</td><td>▂▂▂▂▂▁▂▃▅█▅▇▆▆▆▃▆▇▅▆▇▆▇█▇▇▇▆▆▆▆▆▆▆▆▅▅▅▅▆</td></tr><tr><td>evaluation_f1</td><td>▁▁▁▁▁▂▂▃▅█▇█▅▇▆▆▆█▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>evaluation_loss</td><td>▁▁▁▁▁▁▁▁▂▂▄▃▅▅▆▅▅▅▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇███████</td></tr><tr><td>evaluation_precision</td><td>▁▁▁▁▁▂▂▇▃▅▅▆▃▆▃▃▃▆▅▅▆▆█████▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>evaluation_precision_maybe</td><td>▁▁▁▁▁▂▃█▁▃▅▅▁▅▁▁▁▅▃▃▅▅█████▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>evaluation_precision_no</td><td>▁▁▁▁▁▁▁▇▇█▇████▇███████████████████▇████</td></tr><tr><td>evaluation_precision_yes</td><td>▁▁▁▁▁▁▁▂▄▇▄█▅▅▅▅▅▆▅▆▆▅▆▆▆▆▆▅▅▅▅▅▆▆▅▅▅▅▅▅</td></tr><tr><td>evaluation_recall</td><td>▁▁▁▁▁▂▂▃▄█▅█▄▆▅▄▅▇▆▆▇▆▇▇▇▇▇▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>learning_rate</td><td>▁▂▃▄▅▆▇▇███▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇▆█▆▆▆▄▂▄▂▂▂▂▁▃▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>399</td></tr><tr><td>evaluation_accuracy</td><td>0.512</td></tr><tr><td>evaluation_f1</td><td>0.34631</td></tr><tr><td>evaluation_loss</td><td>1.82987</td></tr><tr><td>evaluation_precision</td><td>0.33825</td></tr><tr><td>evaluation_precision_maybe</td><td>0.0</td></tr><tr><td>evaluation_precision_no</td><td>0.4878</td></tr><tr><td>evaluation_precision_yes</td><td>0.52695</td></tr><tr><td>evaluation_recall</td><td>0.36941</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.05874</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">exalted-glade-71</strong>: <a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/3rbhuy90\" target=\"_blank\">https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/3rbhuy90</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220430_012227-3rbhuy90/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3rbhuy90). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nonraidv/home/CS5520_1/vijeta_trial/bioQA/pubmedqa/wandb/run-20220430_033955-1j7fu23l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/1j7fu23l\" target=\"_blank\">dulcet-frog-72</a></strong> to <a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/CS5520_1/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/2e65addecca4197502cd10ab8ef1919a47c28672f62d7abac7cc9afdcf24fb2d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 369.93it/s]\n",
      " 47%|██████████████████████████████████████▊                                            | 187/400 [1:04:04<1:12:43, 20.49s/it]wandb: Network error (ReadTimeout), entering retry loop.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 400/400 [2:17:09<00:00, 20.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====\n",
      "Results for model\t : ./results\n",
      "=====\n",
      "Precision \t\t = 0.507347\n",
      "Recall \t\t\t = 0.385043\n",
      "f1-score \t\t = 0.377364\n",
      "Accuracy \t\t = 0.544000\n",
      "=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "for model_idx in model_dict:\n",
    "    print('\\nStarting training of model: %s'%(model_dict[model_idx]['model']))\n",
    "    \n",
    "    #\n",
    "    args['model'] = model_dict[model_idx]['model']\n",
    "    wandb.init(\n",
    "        project='Bio-Med-Clinical QA', \n",
    "        config=args\n",
    "    )\n",
    "    \n",
    "    # get dataloaders for training and testing\n",
    "    dataloaders = QADataLoader(\n",
    "        datasets_name='pubmed_qa',\n",
    "        datasets_config='pqa_labeled',\n",
    "        label2id=label2id,\n",
    "        tokenizer_name=model_dict[model_idx]['tokenizer'],\n",
    "        max_sequence_length=args['max_sequence_length'],\n",
    "        batch_size=args['batch_size'],\n",
    "        debug=False\n",
    "    )\n",
    "    #inspect_dataloader(dataloaders)\n",
    "\n",
    "    #\n",
    "    train_loader = dataloaders.dataloader_train\n",
    "    val_loader = dataloaders.dataloader_validation\n",
    "    test_loader = dataloaders.dataloader_test\n",
    "    \n",
    "    # set total steps and warmp-up steps for sheduler\n",
    "    args['t_total'] = len(train_loader) // args['gradient_accumulation_steps'] * args['epochs']\n",
    "    args['warmup_steps'] = int(0.10*args['t_total'])\n",
    "\n",
    "    # define model\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dict[model_idx]['model'], \n",
    "        config=config,\n",
    "    )\n",
    "    \"\"\"\n",
    "    #\n",
    "    model = QAModel(\n",
    "        model_name=model_dict[model_idx]['model'],\n",
    "        num_classes=dataloaders.num_classes,\n",
    "    )\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier.weight' in name:\n",
    "            torch.nn.init.zeros_(param.data)\n",
    "        elif 'classifier.bias' in name:\n",
    "            param.data = torch.tensor([class_dist['yes'], class_dist['no'], class_dist['maybe']]).float()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        get_grouped_parameters(model, no_decay, args['weight_decay']), \n",
    "        lr=args['learning_rate'], \n",
    "        eps=args['adam_epsilon']\n",
    "    )\n",
    "\n",
    "    # scheduler for lr\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=args['warmup_steps'],\n",
    "        num_training_steps=args['t_total']\n",
    "    )\n",
    "\n",
    "    # objective function\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    \n",
    "    # train\n",
    "    best_model = None\n",
    "    best_f1_eval = -1\n",
    "    best_test_results = None\n",
    "    best_val_results = None\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for each_epoch in tqdm(range(args['epochs'])):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            global_step += 1\n",
    "\n",
    "            # clean gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # unroll inputs and sent to device\n",
    "            input_batch = {\n",
    "                'input_ids': batch['input_ids'],\n",
    "                'attention_mask': batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "\n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fct(\n",
    "                logits, \n",
    "                batch['encoder_labels'].to(device)\n",
    "            )\n",
    "            \n",
    "            # log info to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": loss,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"epoch\": each_epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "            \n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters and lr\n",
    "            optimizer.step()\n",
    "            scheduler.step()  \n",
    "\n",
    "            if global_step%args['eval_every_steps'] == 0:\n",
    "                # evaluate model\n",
    "                val_predictions = evaluate(\n",
    "                    model=model, \n",
    "                    data_loader=val_loader,\n",
    "                    objective_f=loss_fct,\n",
    "                )\n",
    "                val_results = get_performance(\n",
    "                    actual_=val_predictions['actual'], \n",
    "                    preds_=val_predictions['preds'], \n",
    "                    dict_mapping=label2id\n",
    "                )\n",
    "\n",
    "                # log info to wandb\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"evaluation_precision\": val_results['metrics']['macro avg']['precision'],\n",
    "                        \"evaluation_recall\": val_results['metrics']['macro avg']['recall'],\n",
    "                        \"evaluation_f1\": val_results['metrics']['macro avg']['f1-score'],\n",
    "                        \"evaluation_accuracy\": val_results['metrics']['accuracy'],\n",
    "                        \"evaluation_loss\": val_predictions['loss'],\n",
    "                        \"epoch\": each_epoch,\n",
    "\n",
    "                        \"evaluation_precision_yes\": val_results['metrics']['0']['precision'],\n",
    "                        \"evaluation_precision_no\": val_results['metrics']['1']['precision'],\n",
    "                        \"evaluation_precision_maybe\": val_results['metrics']['2']['precision'],\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "\n",
    "                # update best model\n",
    "                if best_f1_eval < val_results['metrics']['weighted avg']['f1-score']:\n",
    "                    best_model = deepcopy(model).to(device)\n",
    "                    best_val_results = deepcopy(val_results)\n",
    "                    best_f1_eval = val_results['metrics']['weighted avg']['f1-score']\n",
    "\n",
    "    \n",
    "    # test the model based on best_model\n",
    "    test_predictions = evaluate(\n",
    "        model=best_model, \n",
    "        data_loader=test_loader,\n",
    "        objective_f=loss_fct,\n",
    "    )\n",
    "    best_test_results = get_performance(\n",
    "        actual_=test_predictions['actual'], \n",
    "        preds_=test_predictions['preds'], \n",
    "        dict_mapping=label2id\n",
    "    )\n",
    "    \n",
    "    # save the results and the model\n",
    "    model_dict[model_idx]['results'] = {\n",
    "        'validation_results': deepcopy(best_val_results),\n",
    "        'test_results': deepcopy(best_test_results),\n",
    "        'trained_model': deepcopy(best_model),\n",
    "    }\n",
    "    \n",
    "    #\n",
    "    print('\\n')\n",
    "    print('='*5)\n",
    "    print('Results for model\\t : %s'%model_dict[model_idx]['model'])\n",
    "    print('='*5)\n",
    "    print('Precision \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['precision'])\n",
    "    print('Recall \\t\\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['recall'])\n",
    "    print('f1-score \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['f1-score'])\n",
    "    print('Accuracy \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['accuracy'])\n",
    "    print('='*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd6866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24727afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff68daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27e447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5036be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0ea8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdafb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a47ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da8fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad64f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618c29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59ebe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42d326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786379d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0d164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7704c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
