{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4b23a6-a674-458b-84da-bc2456887356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/vijeta/miniconda3/envs/bioqa/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "from PubMedQAData import QADataLoader\n",
    "import wandb\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddebc702-70e9-4e82-a31a-b32030429e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            finetuning_task='pubmedqa'\n",
    "        )\n",
    "        self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(**batch_)\n",
    "        #pooled = torch.mean(outputs[0], dim=1).to(device)\n",
    "        #logits_ = self.classifier(pooled)\n",
    "        logits_ = outputs[0]\n",
    "        \n",
    "        return logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0cca134-3065-4661-8c81-ed66798a8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for collecting all predictions on the input dataset\n",
    "def get_predictions(model_, loader_):\n",
    "    model_.eval()\n",
    "    \n",
    "    #\n",
    "    dict_results = {}\n",
    "    all_preds = []\n",
    "    for batch_idx, batch_ in tqdm(enumerate(loader_)):\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            # unroll features\n",
    "            input_batch = {\n",
    "                'input_ids':batch_['input_ids'],\n",
    "                'attention_mask':batch_['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "            \n",
    "            # update\n",
    "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "            all_preds += preds\n",
    "            ids_ = batch_['ids'].numpy().tolist()\n",
    "            for id_idx, id_ in enumerate(ids_):\n",
    "                dict_results[str(id_)] = {'custom_label': preds[id_idx]}\n",
    "    \n",
    "    # get distribution of predicted labels\n",
    "    count = {}\n",
    "    count['yes'] = (np.array(all_preds) == 0).sum()\n",
    "    count['no'] = (np.array(all_preds) == 1).sum()\n",
    "    count['maybe'] = (np.array(all_preds) == 2).sum()\n",
    "    dist_class = {}\n",
    "    for i in ['yes', 'no', 'maybe']:\n",
    "        dist_class[i] = count[i]/len(all_preds)\n",
    "    \n",
    "    return dict_results, dist_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f16cba2-9559-4b62-b67f-d50976bd1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we get the data with artificial label we will need to convert it back to the required format, following class does that\n",
    "\n",
    "class CustomArtiDataloader():\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dict_data: dict,\n",
    "        label2id: dict,\n",
    "        batch_size: int = 16,\n",
    "        debug: bool = False,\n",
    "        debug_size: int = 8,\n",
    "    ):\n",
    "        data = self.to_list(dict_data)\n",
    "        \n",
    "        # define Dataset object\n",
    "        self.dataset = CustomArtiDataset(data)\n",
    "        \n",
    "        # define dataloader object\n",
    "        self.dataloader = Dataloader(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collation_f,            \n",
    "        )\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def to_list(self, data_in):\n",
    "        \n",
    "        data_out = []\n",
    "        for idx_ in range(len(data_in['input_ids'])):\n",
    "            instance = {k_: v_[idx_] for k_, v_ in data_in.items()}\n",
    "            data_out.append(instance)\n",
    "            \n",
    "        return data_out\n",
    "    \n",
    "    def collation_f(self, batch):\n",
    "        \n",
    "        #\n",
    "        input_ids_list = [ex[\"input_ids\"] for ex in batch]\n",
    "        attention_mask_list = [ex[\"attention_mask\"] for ex in batch]\n",
    "        decoder_input_ids_list = [ex[\"decoder_input_ids\"] for ex in batch]\n",
    "        decoder_attention_mask_list = [ex[\"decoder_attention_mask\"] for ex in batch]\n",
    "        decoder_labels_list = [ex[\"decoder_labels\"] for ex in batch]\n",
    "        encoder_label_list = [ex['encoder_labels_artificial'] for ex in batch]\n",
    "\n",
    "        collated_batch = {\n",
    "            \"input_ids\": torch.LongTensor(input_ids_list),\n",
    "            \"attention_mask\": torch.LongTensor(attention_mask_list),\n",
    "            \"encoder_labels\": torch.LongTensor(encoder_label_list),\n",
    "            \"decoder_input_ids\": torch.LongTensor(decoder_input_ids_list),\n",
    "            \"decoder_attention_mask\": torch.LongTensor(decoder_attention_mask_list),\n",
    "            \"decoder_labels\": torch.LongTensor(decoder_labels_list),\n",
    "        }\n",
    "\n",
    "        return collated_batch\n",
    "    \n",
    "class CustomArtiDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, list_data):\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return list_data[idx]\n",
    "\n",
    "#\n",
    "def inspect_dataloader(loaders):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    #\n",
    "    print(f\"\\nSize of the training set is {len(loaders.dataset_train)}\")\n",
    "    print(f\"Size of the validation set is {len(loaders.dataset_validation)}\")\n",
    "    print(f\"Size of the test set is {len(loaders.dataset_test)}\")\n",
    "    \n",
    "    #\n",
    "    check_first = loaders.dataset_validation[0]['input_ids'] == loaders.dataset_test[0]['input_ids']\n",
    "    check_last = loaders.dataset_validation[-1]['input_ids'] == loaders.dataset_test[-1]['input_ids']\n",
    "    print(f\"\\nFirst example in test and validation set is same: {check_first}\")\n",
    "    print(f\"Last example in test and validation set is same: {check_last}\")\n",
    "    \n",
    "    # check if train example exists in test or validation set\n",
    "    with open('test_set.json', 'r') as f:\n",
    "        test_ = json.load(f)\n",
    "    with open('dev_set.json', 'r') as f:\n",
    "        dev_ = json.load(f)\n",
    "    check_pool = list(test_.keys()) + list(dev_.keys())\n",
    "    \n",
    "    \n",
    "    # check distribution of all classes in train, test and valid\n",
    "    id2label = {0: 'yes', 1: 'no', 2: 'maybe'}\n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_train))):\n",
    "        label_i = loaders.dataset_train[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in training set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_train)}\")\n",
    "        \n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_validation))):\n",
    "        label_i = loaders.dataset_validation[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in validation set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_validation)}\")\n",
    "    \n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_test))):\n",
    "        label_i = loaders.dataset_test[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in test set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_test)}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\nChecking if training examples exists in test.dev set...\")\n",
    "    for idx in range(len(loaders.dataset_train)):\n",
    "        train_i = loaders.dataset_train[idx]\n",
    "        id_ = train_i['id'][0]\n",
    "        assert id_ not in check_pool, \"Training exampl exists in test/dev set, check dataloader\"\n",
    "    \n",
    "    #\n",
    "    print(\"\\nPrinting three randomly sampled examples...\")\n",
    "    random_samples = np.random.randint(0, len(loaders.dataset_train), size=3)\n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = loaders.dataset_train[sample_]\n",
    "        tokenizer = loaders.source_tokenizer\n",
    "        id2label = loaders.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('Gold label is as follows:')\n",
    "        print(id2label[tokenized_sample['gold_label'][0]])\n",
    "    \"\"\"\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d6789a-c637-4347-bddf-0a8ed73a8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2:\n",
    "# Step 1: get dataloader for unlabled and artifial dataset\n",
    "# Step 2: instantiate biomed-roberta model and load previously trained model\n",
    "# Step 3: use loaded model to predict artificial labels\n",
    "# Step 4: convert the predictions into dataloader\n",
    "# Step 5: train BioMedRoberta on artificial data\n",
    "# Step 6: save the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02a7cd01-f0ee-4812-951a-ce18dc4d04c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel_dict = {\\n    0: {\\n        'model': 'RoBERTa-large-PM-M3-hf',\\n        'tokenizer': 'roberta-large',\\n    },\\n}\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 10,\n",
    "    'learning_rate': 6.2e-6,\n",
    "    'epochs': 100,\n",
    "    'eval_every_steps': 300,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 768,\n",
    "    'output_dir': r'./local_biomed_roberta_base',\n",
    "}\n",
    "label2id = {\n",
    "    'yes': 0,\n",
    "    'no': 1,\n",
    "    'maybe': 2,\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "#\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'allenai/biomed_roberta_base',\n",
    "        'tokenizer': 'allenai/biomed_roberta_base',\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'RoBERTa-large-PM-M3-hf',\n",
    "        'tokenizer': 'roberta-large',\n",
    "    },\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7983248f-7bd3-4629-885a-527bd6025e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading unlabeled and artificially labeled subsets of the data\n",
      "length of artificially labled data: 211269\n",
      "length of unlabeled data: 61249\n",
      "length of combined data: 272518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272518it [00:00, 320960.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Dataloader\n",
    "\n",
    "#\n",
    "data_all = QADataLoader(\n",
    "    datasets_name=None,#'pubmed_qa',\n",
    "    datasets_config=None,#'pqa_artificial',\n",
    "    label2id=label2id,\n",
    "    tokenizer_name=model_dict[0]['tokenizer'],\n",
    "    max_sequence_length=args['max_sequence_length'],\n",
    "    batch_size=args['batch_size'],\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89b978e-3176-4d88-bce4-bba1224d667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect_dataloader(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327259f4-6fef-4b95-933d-755bd165ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QAModel(\n",
       "  (encoder): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Model\n",
    "\n",
    "#\n",
    "model_name = model_dict[0]['model'].split('/')[-1]\n",
    "model = QAModel(\n",
    "    model_name= model_dict[0]['model'],\n",
    "    num_classes=data_all.num_classes,\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(args['output_dir'],  model_name+'_phase1_.pt')))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a18ad78-a56d-4e28-ae92-363539823d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "355it [51:23,  8.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Predict (get artificial labels)\n",
    "predictions, dist_pred = get_predictions(model, data_all.dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5cd037-0ecd-469f-a686-c6721e9faf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 0.8312328968392441, 'no': 0.12620984774743338, 'maybe': 0.04255725541332256}\n"
     ]
    }
   ],
   "source": [
    "print(dist_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6fc358-3c08-44b7-9514-b20e7ca50fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'yes',\n",
    "    1: 'no',\n",
    "    2: 'maybe',\n",
    "}\n",
    "\n",
    "model_labeled_data = {}\n",
    "\n",
    "with open('ori_pqaa.json', 'r') as f:\n",
    "    a_ = json.load(f)\n",
    "with open('ori_pqau.json', 'r') as f:\n",
    "    u_ = json.load(f)\n",
    "\n",
    "for data_ in [a_, u_]:\n",
    "    for id_idx, id_ in enumerate(data_):\n",
    "        model_labeled_data[id_] = data_[id_]\n",
    "        if id_ in predictions:\n",
    "            model_labeled_data[id_]['custom_label'] = id2label[predictions[id_]['custom_label']]\n",
    "        else:\n",
    "            model_labeled_data[id_]['custom_label'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adf568f0-f67b-4b84-901a-c347e3bf9f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QUESTION': 'Are group 2 innate lymphoid cells ( ILC2s ) increased in chronic rhinosinusitis with nasal polyps or eosinophilia?',\n",
       " 'CONTEXTS': ['Chronic rhinosinusitis (CRS) is a heterogeneous disease with an uncertain pathogenesis. Group 2 innate lymphoid cells (ILC2s) represent a recently discovered cell population which has been implicated in driving Th2 inflammation in CRS; however, their relationship with clinical disease characteristics has yet to be investigated.',\n",
       "  'The aim of this study was to identify ILC2s in sinus mucosa in patients with CRS and controls and compare ILC2s across characteristics of disease.',\n",
       "  'A cross-sectional study of patients with CRS undergoing endoscopic sinus surgery was conducted. Sinus mucosal biopsies were obtained during surgery and control tissue from patients undergoing pituitary tumour resection through transphenoidal approach. ILC2s were identified as CD45(+) Lin(-) CD127(+) CD4(-) CD8(-) CRTH2(CD294)(+) CD161(+) cells in single cell suspensions through flow cytometry. ILC2 frequencies, measured as a percentage of CD45(+) cells, were compared across CRS phenotype, endotype, inflammatory CRS subtype and other disease characteristics including blood eosinophils, serum IgE, asthma status and nasal symptom score.',\n",
       "  '35 patients (40% female, age 48 Â± 17 years) including 13 with eosinophilic CRS (eCRS), 13 with non-eCRS and 9 controls were recruited. ILC2 frequencies were associated with the presence of nasal polyps (P = 0.002) as well as high tissue eosinophilia (P = 0.004) and eosinophil-dominant CRS (P = 0.001) (Mann-Whitney U). They were also associated with increased blood eosinophilia (P = 0.005). There were no significant associations found between ILC2s and serum total IgE and allergic disease. In the CRS with nasal polyps (CRSwNP) population, ILC2s were increased in patients with co-existing asthma (P = 0.03). ILC2s were also correlated with worsening nasal symptom score in CRS (P = 0.04).'],\n",
       " 'LABELS': ['BACKGROUND', 'OBJECTIVE', 'METHODS', 'RESULTS'],\n",
       " 'LONG_ANSWER': 'As ILC2s are elevated in patients with CRSwNP, they may drive nasal polyp formation in CRS. ILC2s are also linked with high tissue and blood eosinophilia and have a potential role in the activation and survival of eosinophils during the Th2 immune response. The association of innate lymphoid cells in CRS provides insights into its pathogenesis.',\n",
       " 'MESHES': ['Adult',\n",
       "  'Aged',\n",
       "  'Antigens, Surface',\n",
       "  'Case-Control Studies',\n",
       "  'Chronic Disease',\n",
       "  'Eosinophilia',\n",
       "  'Female',\n",
       "  'Humans',\n",
       "  'Hypersensitivity',\n",
       "  'Immunity, Innate',\n",
       "  'Immunoglobulin E',\n",
       "  'Immunophenotyping',\n",
       "  'Leukocyte Count',\n",
       "  'Lymphocyte Subsets',\n",
       "  'Male',\n",
       "  'Middle Aged',\n",
       "  'Nasal Mucosa',\n",
       "  'Nasal Polyps',\n",
       "  'Neutrophil Infiltration',\n",
       "  'Patient Outcome Assessment',\n",
       "  'Rhinitis',\n",
       "  'Sinusitis',\n",
       "  'Young Adult'],\n",
       " 'final_decision': 'yes',\n",
       " 'custom_label': 'yes'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_labeled_data['25429730']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5603c5d-60c1-4005-bbac-95f6961da00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_labeled_data.json', 'w') as f:\n",
    "    json.dump(model_labeled_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08e8ac8d-24a7-4134-84d9-78d636f4254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_labeled_data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb71273d-9df3-45cd-a462-afdc3c06375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in data:\n",
    "    if data[i_]['custom_label'] != '':\n",
    "        count[data[i_]['custom_label']] += 1\n",
    "\n",
    "dist_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in count:\n",
    "    dist_[i_] = count[i_] / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ebb41be-cb17-4074-8c40-0c20e28da03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yes': 0.8304001937486698,\n",
       " 'no': 0.12608341467352616,\n",
       " 'maybe': 0.04251462288729552}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "299dc028-abd3-4702-8c42-4883edecbc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 0.8268179430015762, 'no': 0.14271379142230994, 'maybe': 0.020466798252464866}\n"
     ]
    }
   ],
   "source": [
    "with open('ori_pqaa_1st_attempt.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "count = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in data:\n",
    "    if data[i_]['custom_label'] != '':\n",
    "        count[data[i_]['custom_label']] += 1\n",
    "\n",
    "dist_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "for i_ in count:\n",
    "    dist_[i_] = count[i_] / len(data)\n",
    "\n",
    "print(dist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318010be-887f-4a7b-b9b7-4d6c81967530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
