{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aece49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8235d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705aeaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/vijeta/miniconda3/envs/bioqa/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dff83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2dee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(split, fold=1):\n",
    "    if split == 'train':\n",
    "        \n",
    "        \"\"\"\n",
    "        train_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/train_set.json' % fold, \n",
    "                                    'r'))\n",
    "        dev_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/dev_set.json' % fold, \n",
    "                                  'r'))\n",
    "        \"\"\"\n",
    "        train_json = json.load(open('train_set.json', 'r'))\n",
    "        dev_json = json.load(open('dev_set.json', 'r'))\n",
    "        \n",
    "        \n",
    "        final_json = {**train_json, **dev_json}\n",
    "    else:\n",
    "        test_json = json.load(open('dev_set.json', 'r')) #json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/test_set.json', 'r'))\n",
    "        final_json = test_json\n",
    "    list_data = []\n",
    "    for key_, val_ in final_json.items():\n",
    "        tmp_ = {'sentence1': val_['QUESTION'], \n",
    "                'sentence2': ' '.join(val_['CONTEXTS']), \n",
    "                'gold_label': val_['final_decision']}\n",
    "        list_data.append(tmp_)\n",
    "    return list_data\n",
    "\n",
    "def read_data_(dict_data_):\n",
    "    \n",
    "    list_data = []\n",
    "    for idx in range(len(dict_data_['question'])):\n",
    "        instance = {\n",
    "            'sentence1': dict_data_['question'][idx],\n",
    "            'sentence2': ''.join(dict_data_['context'][idx]['contexts']),\n",
    "            'gold_label': dict_data_['final_decision'][idx]\n",
    "        }\n",
    "        list_data.append(instance)\n",
    "    \n",
    "    return list_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629018c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_wts(dict_cnt, alpha=15):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = np.log(alpha * tot_cnt/dict_cnt[each_cat])\n",
    "    return wt_\n",
    "\n",
    "def get_class_dist(dict_cnt):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = dict_cnt[each_cat]/tot_cnt\n",
    "    return wt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3588c44b-c86e-4d82-9cd7-9317a473b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/users/vijeta/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 381.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pubid', 'question', 'context', 'long_answer', 'final_decision'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pubmedqa = datasets.load_dataset('pubmed_qa', 'pqa_labeled')\n",
    "pubmedqa_train, pubmedqa_test = train_test_split(pubmedqa['train'])\n",
    "\n",
    "pubmedqa_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f827a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_data = {}\n",
    "dict_data['train'] = read_data(split='train', fold=1)\n",
    "dict_data['test'] = read_data(split='test')\n",
    "#dict_data['train'] = read_data_(pubmedqa_train)\n",
    "#dict_data['test'] = read_data_(pubmedqa_test)\n",
    "\n",
    "label2id = {'yes':0, 'no': 1, 'maybe': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6815ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': \"Is cytokeratin immunoreactivity useful in the diagnosis of short-segment Barrett's oesophagus in Korea?\",\n",
       " 'sentence2': \"Cytokeratin 7/20 staining has been reported to be helpful in diagnosing Barrett's oesophagus and gastric intestinal metaplasia. However, this is still a matter of some controversy. To determine the diagnostic usefulness of cytokeratin 7/20 immunostaining for short-segment Barrett's oesophagus in Korea. In patients with Barrett's oesophagus, diagnosed endoscopically, at least two biopsy specimens were taken from just below the squamocolumnar junction. If goblet cells were found histologically with alcian blue staining, cytokeratin 7/20 immunohistochemical stains were performed. Intestinal metaplasia at the cardia was diagnosed whenever biopsy specimens taken from within 2 cm below the oesophagogastric junction revealed intestinal metaplasia. Barrett's cytokeratin 7/20 pattern was defined as cytokeratin 20 positivity in only the superficial gland, combined with cytokeratin 7 positivity in both the superficial and deep glands. Barrett's cytokeratin 7/20 pattern was observed in 28 out of 36 cases (77.8%) with short-segment Barrett's oesophagus, 11 out of 28 cases (39.3%) with intestinal metaplasia at the cardia, and nine out of 61 cases (14.8%) with gastric intestinal metaplasia. The sensitivity and specificity of Barrett's cytokeratin 7/20 pattern were 77.8 and 77.5%, respectively.\",\n",
       " 'gold_label': 'yes'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c290a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Train\n",
      "====================\n",
      "Train:  Counter({'yes': 276, 'no': 169, 'maybe': 55})\n",
      "Train:  93.272\n",
      "Train:  1330.376\n",
      "\n",
      "\n",
      "====================\n",
      "Test\n",
      "====================\n",
      "Test:  Counter({'yes': 27, 'no': 17, 'maybe': 6})\n",
      "Test:  91.56\n",
      "Test:  1295.32\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*10)\n",
    "print('Train')\n",
    "print(\"==\"*10)\n",
    "class_counts = Counter([x['gold_label'] for x in dict_data['train']])\n",
    "print(\"Train: \", Counter([x['gold_label'] for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence1'].__len__() for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence2'].__len__() for x in dict_data['train']]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"==\"*10)\n",
    "print(\"Test\")\n",
    "print(\"==\"*10)\n",
    "print(\"Test: \", Counter([x['gold_label'] for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence1'].__len__() for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence2'].__len__() for x in dict_data['test']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8aef64-9d0c-4553-9c54-6834a7df1d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b3c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 1.6928195213731514, 'no': 2.183321672167228, 'maybe': 3.3058872018578307}\n",
      "{'yes': 0.552, 'no': 0.338, 'maybe': 0.11}\n"
     ]
    }
   ],
   "source": [
    "#class_wts = get_class_wts(dict_cnt={'yes': 276, 'no': 169, 'maybe': 55}, \n",
    "#                          alpha=3)\n",
    "\n",
    "class_wts = get_class_wts(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }, \n",
    "    alpha=3\n",
    ")\n",
    "print(class_wts)\n",
    "\n",
    "class_dist = get_class_dist(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }\n",
    ")\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4cda53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            finetuning_task='pubmedqa'\n",
    "        )\n",
    "        self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(**batch_)\n",
    "        #pooled = torch.mean(outputs[0], dim=1).to(device)\n",
    "        #logits_ = self.classifier(pooled)\n",
    "        logits_ = outputs[0]\n",
    "        \n",
    "        return logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17afa8a6-08e4-4460-be7c-7d1f0922ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilliary functions\n",
    "\n",
    "def inspect_dataloader(loaders):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    #\n",
    "    print(f\"\\nSize of the training set is {len(loaders.dataset_train)}\")\n",
    "    print(f\"Size of the validation set is {len(loaders.dataset_validation)}\")\n",
    "    print(f\"Size of the test set is {len(loaders.dataset_test)}\")\n",
    "    \n",
    "    #\n",
    "    check_first = loaders.dataset_validation[0]['input_ids'] == loaders.dataset_test[0]['input_ids']\n",
    "    check_last = loaders.dataset_validation[-1]['input_ids'] == loaders.dataset_test[-1]['input_ids']\n",
    "    print(f\"\\nFirst example in test and validation set is same: {check_first}\")\n",
    "    print(f\"Last example in test and validation set is same: {check_last}\")\n",
    "    \n",
    "    # check if train example exists in test or validation set\n",
    "    with open('test_set.json', 'r') as f:\n",
    "        test_ = json.load(f)\n",
    "    with open('dev_set.json', 'r') as f:\n",
    "        dev_ = json.load(f)\n",
    "    check_pool = list(test_.keys()) + list(dev_.keys())\n",
    "    \n",
    "    \n",
    "    # check distribution of all classes in train, test and valid\n",
    "    id2label = {0: 'yes', 1: 'no', 2: 'maybe'}\n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_train))):\n",
    "        label_i = loaders.dataset_train[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in training set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_train)}\")\n",
    "        \n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_validation))):\n",
    "        label_i = loaders.dataset_validation[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in validation set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_validation)}\")\n",
    "    \n",
    "    count_ = {'yes': 0, 'no': 0, 'maybe': 0}\n",
    "    for idx in tqdm(range(len(loaders.dataset_test))):\n",
    "        label_i = loaders.dataset_test[idx]['gold_label'][0]\n",
    "        label_i = id2label[label_i]\n",
    "        count_[label_i] += 1\n",
    "    print(\"Distribution of classes in test set\")\n",
    "    for c_ in count_:\n",
    "        print(f\"Class: {c_}, Percentage: {count_[c_] / len(loaders.dataset_test)}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\nChecking if training examples exists in test.dev set...\")\n",
    "    for idx in range(len(loaders.dataset_train)):\n",
    "        train_i = loaders.dataset_train[idx]\n",
    "        id_ = train_i['id'][0]\n",
    "        assert id_ not in check_pool, \"Training exampl exists in test/dev set, check dataloader\"\n",
    "    \n",
    "    #\n",
    "    print(\"\\nPrinting three randomly sampled examples...\")\n",
    "    random_samples = np.random.randint(0, len(loaders.dataset_train), size=3)\n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = loaders.dataset_train[sample_]\n",
    "        tokenizer = loaders.source_tokenizer\n",
    "        id2label = loaders.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('Gold label is as follows:')\n",
    "        print(id2label[tokenized_sample['gold_label'][0]])\n",
    "    \"\"\"\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_grouped_parameters(\n",
    "    model_in, \n",
    "    no_decay_layers, \n",
    "    weight_decay\n",
    "):\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model_in.named_parameters() if not any(nd in n for nd in no_decay_layers)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model_in.named_parameters() if any(nd in n for nd in no_decay_layers)], \n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "def evaluate(model, data_loader, objective_f):\n",
    "    model.eval()\n",
    "    dict_result = {'actual':[],\n",
    "                   'preds':[]}\n",
    "    \n",
    "    #print('\\nStarting model evaluation:')\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            \n",
    "            # unroll features\n",
    "            dict_result['actual'] += batch['encoder_labels'].numpy().tolist()\n",
    "            input_batch = {\n",
    "                'input_ids':batch['input_ids'],\n",
    "                'attention_mask':batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "            \n",
    "            # calculate loss\n",
    "            #print(logits.shape)\n",
    "            #print(batch['encoder_labels'].shape)\n",
    "            eval_loss += objective_f(logits, batch['encoder_labels'].to(device)).item()\n",
    "            \n",
    "            # update\n",
    "            dict_result['preds'] += np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "    \n",
    "    # update\n",
    "    dict_result['actual'] = [x for x in dict_result['actual']]\n",
    "    dict_result['loss'] = eval_loss / (batch_idx + 1)\n",
    "    \n",
    "    return dict_result\n",
    "\n",
    "def get_performance(\n",
    "    actual_, \n",
    "    preds_,\n",
    "    dict_mapping\n",
    "):\n",
    "    results = {}\n",
    "    \n",
    "    # accuracy, precision, recall, f1\n",
    "    results['metrics'] = classification_report(\n",
    "        actual_, \n",
    "        preds_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    for name_, cls_ in dict_mapping.items():\n",
    "        if not str(cls_) in results['metrics']:\n",
    "            results['metrics'][str(cls_)] = {'precision': 0}\n",
    "            print(f\"\\nUnique gold labels in the current batch are: {list(set(actual_))}\")\n",
    "            print(f\"Unique predicted labels are: {list(set(preds_))}\")\n",
    "    \n",
    "    # confusion matrix\n",
    "    results['confusion_matrix'] = pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            actual_, \n",
    "            preds_\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # counter\n",
    "    results['actual_counter'] = Counter(actual_)\n",
    "    results['prediction_counter'] = Counter(preds_)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaab0f-98e9-4536-b861-a91d27f73841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec39a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 10,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 100,\n",
    "    'eval_every_steps': 100,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 8,\n",
    "    'scheduler_warmup': 0.2,\n",
    "    'training_phase': 'phase3',\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c052db2e-c4f9-45ca-96aa-302b9a4f987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from PubMedQAData import QADataLoader\n",
    "label2id = {'yes': 0, 'no': 1, 'maybe': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c68844-0ef8-4f4f-b6ca-f51384ec9476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    1: {\\n        'model': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\\n        'tokenizer': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\\n    }\\n}\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'allenai/biomed_roberta_base',\n",
    "        'tokenizer': 'allenai/biomed_roberta_base',\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "    1: {\n",
    "        'model': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "        'tokenizer': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83bdfc61-56ab-4306-918f-2993d43ff5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel_dict = {\\n    0: {\\n        'model': 'RoBERTa-large-PM-M3/RoBERTa-large-PM-M3-hf',\\n        'tokenizer': 'roberta-large',\\n    },\\n}\\n\\n    1: {\\n        'model': 'dmis-lab/biobert-large-cased-v1.1',\\n        'tokenizer': 'dmis-lab/biobert-large-cased-v1.1',\\n    },    \\n    2: {\\n        'model': 'healx/biomedical-slot-filling-reader-large',\\n        'tokenizer': 'healx/biomedical-slot-filling-reader-large',\\n    }\\n}\\n\\n\\nmodel_dict = {\\n    0: {\\n        'model': 'prajjwal1/bert-tiny',\\n        'tokenizer': 'prajjwal1/bert-tiny'\\n    },\\n}\\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'RoBERTa-large-PM-M3/RoBERTa-large-PM-M3-hf',\n",
    "        'tokenizer': 'roberta-large',\n",
    "    },\n",
    "}\n",
    "\n",
    "    1: {\n",
    "        'model': 'dmis-lab/biobert-large-cased-v1.1',\n",
    "        'tokenizer': 'dmis-lab/biobert-large-cased-v1.1',\n",
    "    },    \n",
    "    2: {\n",
    "        'model': 'healx/biomedical-slot-filling-reader-large',\n",
    "        'tokenizer': 'healx/biomedical-slot-filling-reader-large',\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'prajjwal1/bert-tiny',\n",
    "        'tokenizer': 'prajjwal1/bert-tiny'\n",
    "    },\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b51771",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr_i in [6e-6]:\n",
    "    args['learning_rate'] = lr_i\n",
    "    for model_idx in model_dict:\n",
    "        print('\\nStarting training of model: %s'%(model_dict[model_idx]['model']))\n",
    "\n",
    "        #\n",
    "        model_name = model_dict[model_idx]['model'].split('/')[-1]\n",
    "        args['output_dir'] = 'local_' + model_name\n",
    "        if not os.path.exists(args['output_dir']):\n",
    "            os.mkdir(args['output_dir'])\n",
    "\n",
    "        #\n",
    "        args['model'] = model_dict[model_idx]['model']\n",
    "        wandb.init(\n",
    "            project='Bio-Med-Clinical-QA-base', \n",
    "            config=args\n",
    "        )\n",
    "\n",
    "        # get dataloaders for training and testing\n",
    "        dataloaders = QADataLoader(\n",
    "            datasets_name='pubmed_qa',\n",
    "            datasets_config='pqa_labeled',\n",
    "            label2id=label2id,\n",
    "            tokenizer_name=model_dict[model_idx]['tokenizer'],\n",
    "            max_sequence_length=args['max_sequence_length'],\n",
    "            batch_size=args['batch_size'],\n",
    "            debug=False\n",
    "        )\n",
    "        inspect_dataloader(dataloaders)\n",
    "        \n",
    "        #\n",
    "        train_loader = dataloaders.dataloader_train\n",
    "        val_loader = dataloaders.dataloader_validation\n",
    "        test_loader = dataloaders.dataloader_test\n",
    "\n",
    "        # set total steps and warmp-up steps for sheduler\n",
    "        args['t_total'] = int(len(train_loader) / args['gradient_accumulation_steps']) * args['epochs']\n",
    "        args['warmup_steps'] = int(args['scheduler_warmup']*args['t_total'])\n",
    "\n",
    "        # define model\n",
    "        \"\"\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_dict[model_idx]['model'], \n",
    "            config=config,\n",
    "        )\n",
    "        \"\"\"\n",
    "        #\n",
    "        model = QAModel(\n",
    "            model_name=model_dict[model_idx]['model'],\n",
    "            num_classes=dataloaders.num_classes,\n",
    "        )\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier.weight' in name:\n",
    "                torch.nn.init.zeros_(param.data)\n",
    "            elif 'classifier.bias' in name:\n",
    "                param.data = torch.tensor([class_dist['yes'], class_dist['no'], class_dist['maybe']]).float()\n",
    "        if args['training_phase'] == 'phase3':\n",
    "            model.load_state_dict(torch.load(os.path.join(args['output_dir'],  f\"{model_name}_phase2_.pt\")))\n",
    "        model = model.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            get_grouped_parameters(model, no_decay, args['weight_decay']), \n",
    "            lr=args['learning_rate'], \n",
    "            eps=args['adam_epsilon']\n",
    "        )\n",
    "\n",
    "        # scheduler for lr\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=args['warmup_steps'],\n",
    "            num_training_steps=args['t_total']\n",
    "        )\n",
    "\n",
    "        # objective function\n",
    "        loss_fct = CrossEntropyLoss(\n",
    "            weight=torch.tensor([class_wts['yes'], class_wts['no'], class_wts['maybe']]).float().to(device), \n",
    "            ignore_index=-100,\n",
    "        )\n",
    "\n",
    "        # train\n",
    "        best_model = None\n",
    "        best_test_results = None\n",
    "        best_f1_eval = 0\n",
    "        best_val_results = None\n",
    "        global_step = 0\n",
    "        loss_log = 0\n",
    "        model.train()\n",
    "        for each_epoch in tqdm(range(args['epochs'])):\n",
    "            model.train()\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "                # unroll inputs and sent to device\n",
    "                input_batch = {\n",
    "                    'input_ids': batch['input_ids'],\n",
    "                    'attention_mask': batch['attention_mask']\n",
    "                }\n",
    "                input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "\n",
    "                # forward pass\n",
    "                logits = model(input_batch)\n",
    "\n",
    "                # calculate loss\n",
    "                loss = loss_fct(logits, batch['encoder_labels'].to(device))\n",
    "                loss_log += loss\n",
    "\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # update parameters and lr\n",
    "                if ((batch_idx + 1) % args['gradient_accumulation_steps'] == 0) or (batch_idx + 1 == len(train_loader)):\n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # par update and clean grads\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    model.zero_grad()\n",
    "                    \n",
    "                    # log info to wandb\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"train/loss\": loss_log/args['gradient_accumulation_steps'],\n",
    "                            \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                            \"epoch\": each_epoch,\n",
    "                        },\n",
    "                        step=global_step,\n",
    "                    )\n",
    "                    \n",
    "                    # update logged value\n",
    "                    loss_log = 0\n",
    "                    \n",
    "                    # update LR \n",
    "                    if ((batch_idx + 1) % args['gradient_accumulation_steps'] == 0):\n",
    "                        scheduler.step()\n",
    "\n",
    "                # evaluation\n",
    "                if global_step%args['eval_every_steps'] == 0:\n",
    "                    # evaluate model\n",
    "                    val_predictions = evaluate(\n",
    "                        model=model, \n",
    "                        data_loader=val_loader,\n",
    "                        objective_f=loss_fct,\n",
    "                    )\n",
    "                    val_results = get_performance(\n",
    "                        actual_=val_predictions['actual'], \n",
    "                        preds_=val_predictions['preds'], \n",
    "                        dict_mapping=label2id\n",
    "                    )\n",
    "\n",
    "                    # log info to wandb\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"eval/precision\": val_results['metrics']['macro avg']['precision'],\n",
    "                            \"eval/recall\": val_results['metrics']['macro avg']['recall'],\n",
    "                            \"eval/f1\": val_results['metrics']['macro avg']['f1-score'],\n",
    "                            \"eval/accuracy\": val_results['metrics']['accuracy'],\n",
    "                            \"eval/loss\": val_predictions['loss'],\n",
    "                            \"epoch\": each_epoch,\n",
    "\n",
    "                            \"eval/precision_yes\": val_results['metrics']['0']['precision'],\n",
    "                            \"eval/precision_no\": val_results['metrics']['1']['precision'],\n",
    "                            \"eval/precision_maybe\": val_results['metrics']['2']['precision'],\n",
    "                        },\n",
    "                        step=global_step,\n",
    "                    )\n",
    "\n",
    "\n",
    "                    # update best model\n",
    "                    if best_f1_eval < val_results['metrics']['weighted avg']['f1-score']:\n",
    "                        #best_model = deepcopy(model).to(device)\n",
    "                        best_val_results = deepcopy(val_results)\n",
    "                        best_f1_eval = val_results['metrics']['weighted avg']['f1-score']\n",
    "\n",
    "                        # save model\n",
    "                        torch.save(model.state_dict(), os.path.join(args['output_dir'], f\"{model_name}_{args['training_phase']}_.pt\"))\n",
    "\n",
    "                    #\n",
    "                    #if val_results['metrics']['2']['precision'] >= 0.5:\n",
    "                    #    # save model\n",
    "                    #    torch.save(model.state_dict(), os.path.join(args['output_dir'],  model_name+'_maybe.pt'))\n",
    "\n",
    "\n",
    "        # test the model based on best_model\n",
    "        model.load_state_dict(torch.load(os.path.join(args['output_dir'],  f\"{model_name}_{args['training_phase']}_.pt\")))\n",
    "        test_predictions = evaluate(\n",
    "            model=model, \n",
    "            data_loader=test_loader,\n",
    "            objective_f=loss_fct,\n",
    "        )\n",
    "        best_test_results = get_performance(\n",
    "            actual_=test_predictions['actual'], \n",
    "            preds_=test_predictions['preds'], \n",
    "            dict_mapping=label2id\n",
    "        )\n",
    "\n",
    "        #\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"test/precision\": best_test_results['metrics']['macro avg']['precision'],\n",
    "                \"test/recall\": best_test_results['metrics']['macro avg']['recall'],\n",
    "                \"test/f1\": best_test_results['metrics']['macro avg']['f1-score'],\n",
    "                \"test/accuracy\": best_test_results['metrics']['accuracy'],\n",
    "                \"epoch\": each_epoch,\n",
    "\n",
    "                \"test/precision_yes\": best_test_results['metrics']['0']['precision'],\n",
    "                \"test/precision_no\": best_test_results['metrics']['1']['precision'],\n",
    "                \"test/precision_maybe\": best_test_results['metrics']['2']['precision'],\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "\n",
    "        # save the results and the model\n",
    "        model_dict[model_idx]['results'] = {\n",
    "            'validation_results': deepcopy(best_val_results),\n",
    "            'test_results': deepcopy(best_test_results),\n",
    "            #'trained_model': deepcopy(best_model),\n",
    "        }\n",
    "\n",
    "        #\n",
    "        print('\\n')\n",
    "        print('='*5)\n",
    "        print('Results for model\\t : %s'%model_dict[model_idx]['model'])\n",
    "        print('='*5)\n",
    "        print('Precision \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['precision'])\n",
    "        print('Recall \\t\\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['recall'])\n",
    "        print('f1-score \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['f1-score'])\n",
    "        print('Accuracy \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['accuracy'])\n",
    "        print('='*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QAModel(\n",
    "    model_name=model_dict[model_idx]['model'],\n",
    "    num_classes=dataloaders.num_classes,\n",
    ")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(os.path.join('local_biomed_roberta_base',  \"biomed_roberta_base_phase1_.pt\")))\n",
    "test_predictions = evaluate(\n",
    "    model=model, \n",
    "    data_loader=test_loader,\n",
    "    objective_f=loss_fct,\n",
    ")\n",
    "best_test_results = get_performance(\n",
    "    actual_=test_predictions['actual'], \n",
    "    preds_=test_predictions['preds'], \n",
    "    dict_mapping=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24727afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff68daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[0]['results']['test_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5036be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0ea8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdafb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a47ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da8fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad64f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618c29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59ebe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42d326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786379d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0d164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7704c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
