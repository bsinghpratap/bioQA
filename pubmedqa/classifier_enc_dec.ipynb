{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aece49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] ='4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8235d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705aeaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/vijeta/miniconda3/envs/bioqa/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    ")\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dff83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2dee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(split, fold=1):\n",
    "    if split == 'train':\n",
    "        train_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/train_set.json' % fold, \n",
    "                                    'r'))\n",
    "        dev_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/dev_set.json' % fold, \n",
    "                                  'r'))\n",
    "        final_json = {**train_json, **dev_json}\n",
    "    else:\n",
    "        test_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/test_set.json', 'r'))\n",
    "        final_json = test_json\n",
    "    list_data = []\n",
    "    for key_, val_ in final_json.items():\n",
    "        tmp_ = {'sentence1': val_['QUESTION'], \n",
    "                'sentence2': ' '.join(val_['CONTEXTS']), \n",
    "                'gold_label': val_['final_decision']}\n",
    "        list_data.append(tmp_)\n",
    "    return list_data\n",
    "\n",
    "def read_data_(dict_data_):\n",
    "    \n",
    "    list_data = []\n",
    "    for idx in range(len(dict_data_['question'])):\n",
    "        instance = {\n",
    "            'sentence1': dict_data_['question'][idx],\n",
    "            'sentence2': ''.join(dict_data_['context'][idx]['contexts']),\n",
    "            'gold_label': dict_data_['final_decision'][idx]\n",
    "        }\n",
    "        list_data.append(instance)\n",
    "    \n",
    "    return list_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629018c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_wts(dict_cnt, alpha=15):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = np.log(alpha * tot_cnt/dict_cnt[each_cat])\n",
    "    return wt_\n",
    "\n",
    "def get_class_dist(dict_cnt):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = dict_cnt[each_cat]/tot_cnt\n",
    "    return wt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3588c44b-c86e-4d82-9cd7-9317a473b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/users/vijeta/.cache/huggingface/datasets/pubmed_qa/pqa_artificial/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 209.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pubid', 'question', 'context', 'long_answer', 'final_decision'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pubmedqa = datasets.load_dataset('pubmed_qa', 'pqa_artificial')\n",
    "pubmedqa_train, pubmedqa_test = train_test_split(pubmedqa['train'])\n",
    "\n",
    "pubmedqa_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f827a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_data = {}\n",
    "#dict_data['train'] = read_data(split='train', fold=1)\n",
    "#dict_data['test'] = read_data(split='test')\n",
    "dict_data['train'] = read_data_(pubmedqa_train)\n",
    "dict_data['test'] = read_data_(pubmedqa_test)\n",
    "\n",
    "label2id = {'yes':0, 'no': 1, 'maybe': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6815ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Do serum levels of vascular dysfunction markers reflect disease severity and stage in systemic sclerosis patients?',\n",
       " 'sentence2': 'To improve knowledge of vasculopathy in SSc through the assessment of serum levels of circulating angiogenetic and endothelial dysfunction markers in patients at different stages of the disease.Sera from 224 subjects were obtained and concentrations of angiopoietin-2, chemokine (C-X-C motif) ligand (CXCL)-16 (CXCL16), E-selectin, soluble intercellular adhesion molecule-1, IL-8 (CXCL8), soluble vascular adhesion molecule-1 and VEGF were determined by a Luminex assay. Subjects included 43 healthy controls, 47 early SSc patients according to LeRoy and Medsger without other signs and symptoms of evolutive disease, 48 definitive SSc (defSSc) patients according to the 2013 ACR/EULAR criteria without skin or lung fibrosis, 51 lcSSc subjects and 35 dcSSc subjects.The four groups of patients showed well-distinct clinical and laboratory characteristics, with a linear decreasing trend in forced vital capacity and diffusing capacity for carbon monoxide % predicted values from early SSc to defSSc to lcSSc and to dcSSc, and a linear increasing trend in ESR, and in the prevalence of abnormal CRP, serum gamma globulins and lung fibrosis (all P < 0.0001). Highly significant linear trends pointing to an increase in angiopoietin-2 (P < 0.0001), CXCL16 (P < 0.0001), E-selectin (P = 0.001) and soluble intercellular adhesion molecule-1 (P = 0.002) in relation to the different disease subsets were observed.',\n",
       " 'gold_label': 'yes'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c290a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Train\n",
      "====================\n",
      "Train:  Counter({'yes': 147132, 'no': 11319})\n",
      "Train:  114.23706382414753\n",
      "Train:  1368.9172867321759\n",
      "\n",
      "\n",
      "====================\n",
      "Test\n",
      "====================\n",
      "Test:  Counter({'yes': 49012, 'no': 3806})\n",
      "Test:  114.36996857132038\n",
      "Test:  1367.2607255102428\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*10)\n",
    "print('Train')\n",
    "print(\"==\"*10)\n",
    "class_counts = Counter([x['gold_label'] for x in dict_data['train']])\n",
    "print(\"Train: \", Counter([x['gold_label'] for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence1'].__len__() for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence2'].__len__() for x in dict_data['train']]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"==\"*10)\n",
    "print(\"Test\")\n",
    "print(\"==\"*10)\n",
    "print(\"Test: \", Counter([x['gold_label'] for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence1'].__len__() for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence2'].__len__() for x in dict_data['test']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8aef64-9d0c-4553-9c54-6834a7df1d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b3c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 1.1727275428860346, 'no': 3.7375749562740017}\n",
      "{'yes': 0.9285646666792888, 'no': 0.07143533332071113}\n"
     ]
    }
   ],
   "source": [
    "#class_wts = get_class_wts(dict_cnt={'yes': 276, 'no': 169, 'maybe': 55}, \n",
    "#                          alpha=3)\n",
    "\n",
    "class_wts = get_class_wts(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        #'maybe': class_counts['maybe'],\n",
    "    }, \n",
    "    alpha=3\n",
    ")\n",
    "print(class_wts)\n",
    "\n",
    "class_dist = get_class_dist(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        #'maybe': class_counts['maybe'],\n",
    "    }\n",
    ")\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4cda53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        #\n",
    "        model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.encoder = model.from_pretrained(model_name)\n",
    "        #self.classifier = nn.Linear(\n",
    "        #    in_features=768,\n",
    "        #    out_features=num_classes,\n",
    "        #)\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=batch_['input_ids'],\n",
    "            attention_mask=batch_['attention_mask'],\n",
    "            decoder_input_ids=batch_['decoder_input_ids'],\n",
    "            decoder_attention_mask=batch_['decoder_attention_mask'],\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        # extract encoder output\n",
    "        encodings = outputs['encoder_last_hidden_state']\n",
    "        pooled = torch.mean(encodings, dim=1)\n",
    "        logits_enc = self.classifier(pooled)\n",
    "        \n",
    "        #\n",
    "        logits_dec = outputs['logits']\n",
    "        \n",
    "        return logits_enc, logits_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17afa8a6-08e4-4460-be7c-7d1f0922ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilliary functions\n",
    "\n",
    "def inspect_dataloader(dataloader_, ):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    random_samples = np.random.randint(0, len(dataloader_.dataset_train), size=3)\n",
    "    \n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = dataloader_.dataset_train[sample_]\n",
    "        source_tokenizer = dataloader_.source_tokenizer\n",
    "        target_tokenizer = dataloader_.target_tokenizer\n",
    "        id2label = dataloader_.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(source_tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('\\nLong answer is as follows:')\n",
    "        print(target_tokenizer.decode(tokenized_sample['decoder_input_ids']))\n",
    "        print('\\nDecoder target is as follows:')\n",
    "        print(target_tokenizer.decode(tokenized_sample['decoder_labels']))\n",
    "        print('\\nEncoder target is as follows:')\n",
    "        print(id2label[tokenized_sample['encoder_label'][0]])        \n",
    "    \n",
    "    return\n",
    "\n",
    "def get_grouped_parameters(\n",
    "    model_in, \n",
    "    no_decay_layers, \n",
    "    weight_decay\n",
    "):\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model_in.named_parameters() if not any(nd in n for nd in no_decay_layers)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model_in.named_parameters() if any(nd in n for nd in no_decay_layers)], \n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "def evaluate(model, data_loader, objective_f):\n",
    "    model.eval()\n",
    "    dict_result = {'actual':[],\n",
    "                   'preds':[]}\n",
    "    \n",
    "    print('\\nStarting model evaluation:')\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in tqdm(enumerate(data_loader)):\n",
    "            \n",
    "            \n",
    "            # unroll features\n",
    "            dict_result['actual'] += batch['encoder_labels'].numpy().tolist()\n",
    "            input_batch = {\n",
    "                'input_ids':batch['input_ids'],\n",
    "                'attention_mask':batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            generated_tokens = model.generate(\n",
    "                input_ids,\n",
    "                bos_token_id=target_tokenizer.bos_token_id,\n",
    "                eos_token_id=target_tokenizer.eos_token_id,\n",
    "                pad_token_id=target_tokenizer.pad_token_id,\n",
    "                #key_padding_mask=key_padding_mask,\n",
    "                max_length=3,\n",
    "                #kind=generation_type,\n",
    "                num_beams=2,\n",
    "            )\n",
    "            preds = target_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            \n",
    "            # calculate loss\n",
    "            #eval_loss += objective_f(\n",
    "            #    logits.permute(0, 2, 1), \n",
    "            #    batch['decoder_labels'].to(device)\n",
    "            #).item()\n",
    "            \n",
    "            # update\n",
    "            dict_result['preds'] += preds#np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "    \n",
    "    # update\n",
    "    dict_result['actual'] = [x for x in dict_result['actual']]\n",
    "    dict_result['loss'] = 0#eval_loss / (batch_idx + 1)\n",
    "    \n",
    "    return dict_result\n",
    "\n",
    "def get_performance(\n",
    "    actual_, \n",
    "    preds_,\n",
    "    dict_mapping\n",
    "):\n",
    "    print('\\nStarting performance evaluation:')\n",
    "    results = {}\n",
    "    \n",
    "    # accuracy, precision, recall, f1\n",
    "    results['metrics'] = classification_report(\n",
    "        actual_, \n",
    "        preds_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    \n",
    "    # confusion matrix\n",
    "    results['confusion_matrix'] = pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            actual_, \n",
    "            preds_\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # counter\n",
    "    results['actual_counter'] = Counter(actual_)\n",
    "    results['prediction_counter'] = Counter(preds_)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaab0f-98e9-4536-b861-a91d27f73841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec39a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'roberta-base'\n",
    "#tokenizer_name = 'roberta-base'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 1,\n",
    "    'learning_rate': 6.5e-6,\n",
    "    'epochs': 1,\n",
    "    'eval_every_steps': 1,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 8,\n",
    "    'wt_classification': 0,\n",
    "    'wt_generation': 1,\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c052db2e-c4f9-45ca-96aa-302b9a4f987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from PubMedQAData_EncDec import QADataLoader\n",
    "label2id = {'yes': 0, 'no': 1, 'maybe': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c68844-0ef8-4f4f-b6ca-f51384ec9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    1: {\n",
    "        'model': 'GanjinZero/biobart-base',\n",
    "        'tokenizer': 'GanjinZero/biobart-base',\n",
    "    },\n",
    "    2: {\n",
    "        'model': r'facebook/bart-base',\n",
    "        'tokenizer': 'facebook/bart-base',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92b51771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training of model: GanjinZero/biobart-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lylz0d6k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">still-shape-2</strong>: <a href=\"https://wandb.ai/vijetakd/BioQA-label-generation/runs/lylz0d6k\" target=\"_blank\">https://wandb.ai/vijetakd/BioQA-label-generation/runs/lylz0d6k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220602_145702-lylz0d6k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lylz0d6k). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nonraidv/home/vijeta/QA_benchmarking/bioQA/pubmedqa/wandb/run-20220602_145817-2kegtv86</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vijetakd/BioQA-label-generation/runs/2kegtv86\" target=\"_blank\">dark-snow-3</a></strong> to <a href=\"https://wandb.ai/vijetakd/BioQA-label-generation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450it [00:00, 166220.77it/s]\n",
      "50it [00:00, 300882.64it/s]\n",
      "500it [00:00, 404153.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting dataloader...\n",
      "\n",
      "Input sequence to the model i.e. Question + Context, is as follows:\n",
      "<s>We investigated the role of surgical ablation targeting the autonomous nervous system during a Cox-Maze IV procedure in the maintenance of sinus rhythm at long-term follow-up. The patient population consisted of 519 subjects with persistent or long-standing persistent atrial fibrillation (AF) undergoing radiofrequency Maze IV during open heart surgery between January 2006 and July 2013 at three institutions without (Group 1) or with (Group 2) ganglionated plexi (GP) ablation. Recurrence of atrial fibrillation off-antiarrhythmic drugs was the primary outcome. Predictors of AF recurrence were evaluated by means of competing risk regression. Median follow-up was 36.7 months. The percentage of patients in normal sinus rhythm (NSR) off-antiarrhythmic drugs did not differ between groups (Group 1-75.5%, Group 2-67.8%, p = 0.08). Duration of AF ≥ 38 months (p = 0.01), left atrial diameter ≥ 54 mm (0.001), left atrial area ≥ 33 cm(2) (p = 0.005), absence of connecting lesions (p= 0.04), and absence of right atrial ablation (p<0.001) were independently associated with high incidence of AF recurrence. In contrast the absence of GP ablation was not a significant factor (p = 0.12).</s></s>Is ganglionated plexi ablation during Maze IV procedure beneficial for postoperative long-term stable sinus rhythm?</s>\n",
      "\n",
      "Long answer is as follows:\n",
      "<s>GP ablation did not prove to be beneficial for postoperative stable NSR. A complete left atrial lesion set and biatrial ablation are advisable for improving rhythm outcomes. Randomized controlled trials are necessary to confirm our findings.</s>\n",
      "\n",
      "Decoder target is as follows:\n",
      "no</s><pad>\n",
      "\n",
      "Encoder target is as follows:\n",
      "no\n",
      "\n",
      "Input sequence to the model i.e. Question + Context, is as follows:\n",
      "<s>To study whether exercise during pregnancy reduces the risk of postnatal depression. Randomized controlled trial. Trondheim and Stavanger University Hospitals, Norway. Eight hundred and fifty-five pregnant women were randomized to intervention or control groups. The intervention was a 12 week exercise program, including aerobic and strengthening exercises, conducted between week 20 and 36 of pregnancy. One weekly group session was led by physiotherapists, and home exercises were encouraged twice a week. Control women received regular antenatal care. Edinburgh Postnatal Depression Scale (EPDS) completed three months after birth. Scores of 10 or more and 13 or more suggested probable minor and major depression, respectively. Fourteen of 379 (3.7%) women in the intervention group and 17 of 340 (5.0%) in the control group had an EPDS score of ≥10 (p=0.46), and four of 379 (1.2%) women in the intervention group and eight of 340 (2.4%) in the control group had an EPDS score of ≥13 (p=0.25). Among women who did not exercise prior to pregnancy, two of 100 (2.0%) women in the intervention group and nine of 95 (9.5%) in the control group had an EPDS score of ≥10 (p=0.03).</s></s>Does exercise during pregnancy prevent postnatal depression?</s>\n",
      "\n",
      "Long answer is as follows:\n",
      "<s>We did not find a lower prevalence of high EPDS scores among women randomized to regular exercise during pregnancy compared with the control group. However, a subgroup of women in the intervention group who did not exercise regularly prior to pregnancy had a reduced risk of postnatal depression.</s>\n",
      "\n",
      "Decoder target is as follows:\n",
      "no</s><pad>\n",
      "\n",
      "Encoder target is as follows:\n",
      "no\n",
      "\n",
      "Input sequence to the model i.e. Question + Context, is as follows:\n",
      "<s>The route of delivery in eclampsia is controversial. We hypothesized that adverse maternal and perinatal outcomes may not be improved by early cesarean delivery. This was a randomized controlled exploratory trial carried out in a rural teaching institution. In all, 200 eclampsia cases, carrying ≥34 weeks, were allocated to either cesarean or vaginal delivery. Composite maternal and perinatal event rates (death and severe morbidity) were compared by intention-to-treat principle. Groups were comparable at baseline with respect to age and key clinical parameters. Maternal event rate was similar: 10.89% in the cesarean arm vs 7.07% for vaginal delivery (relative risk, 1.54; 95% confidence interval, 0.62-3.81). Although the neonatal event rate was less in cesarean delivery-9.90% vs 19.19% (relative risk, 0.52; 95% confidence interval, 0.25-1.05)-the difference was not significant statistically.</s></s>Does route of delivery affect maternal and perinatal outcome in women with eclampsia?</s>\n",
      "\n",
      "Long answer is as follows:\n",
      "<s>A policy of early cesarean delivery in eclampsia, carrying ≥34 weeks, is not associated with better outcomes.</s>\n",
      "\n",
      "Decoder target is as follows:\n",
      "no</s><pad>\n",
      "\n",
      "Encoder target is as follows:\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "for model_idx in model_dict:\n",
    "    print('\\nStarting training of model: %s'%(model_dict[model_idx]['model']))\n",
    "    \n",
    "    #\n",
    "    args['model'] = model_dict[model_idx]['model']\n",
    "    wandb.init(\n",
    "        project='BioQA-label-generation', \n",
    "        config=args\n",
    "    )\n",
    "    \n",
    "    # get dataloaders for training and testing\n",
    "    dataloaders = QADataLoader(\n",
    "        datasets_name='pubmed_qa',\n",
    "        datasets_config='pqa_labeled',\n",
    "        label2id=label2id,\n",
    "        tokenizer_name=model_dict[model_idx]['tokenizer'],\n",
    "        max_sequence_length=args['max_sequence_length'],\n",
    "        batch_size=args['batch_size'],\n",
    "        debug=False\n",
    "    )\n",
    "    inspect_dataloader(dataloaders)\n",
    "    break\n",
    "\n",
    "    #\n",
    "    train_loader = dataloaders.dataloader_train\n",
    "    val_loader = dataloaders.dataloader_validation\n",
    "    test_loader = dataloaders.dataloader_test\n",
    "    \n",
    "    # set total steps and warmp-up steps for sheduler\n",
    "    args['t_total'] = len(train_loader) // args['gradient_accumulation_steps'] * args['epochs']\n",
    "    args['warmup_steps'] = int(0.10*args['t_total'])\n",
    "\n",
    "    # define model\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dict[model_idx]['model'], \n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier.weight' in name:\n",
    "            torch.nn.init.zeros_(param.data)\n",
    "        elif 'classifier.bias' in name:\n",
    "            param.data = torch.tensor([class_dist['yes'], class_dist['no'], class_dist['maybe']]).float()\n",
    "    \"\"\"\n",
    "    #\n",
    "    model = QAModel(\n",
    "        model_name=model_dict[model_idx]['model'],\n",
    "        num_classes=dataloaders.num_classes,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        get_grouped_parameters(model, no_decay, args['weight_decay']), \n",
    "        lr=args['learning_rate'], \n",
    "        eps=args['adam_epsilon']\n",
    "    )\n",
    "\n",
    "    # scheduler for lr\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=args['warmup_steps'],\n",
    "        num_training_steps=args['t_total']\n",
    "    )\n",
    "\n",
    "    # objective function\n",
    "    loss_fct = CrossEntropyLoss(weight=torch.tensor(list(class_wts.values())).float().to(device), ignore_index=-100)\n",
    "    loss_fct_dec = CrossEntropyLoss(ignore_index=-100)\n",
    "    \n",
    "    # progress bar\n",
    "    progress_bar = tqdm(range(int((len(train_loader)/args['batch_size'])*args['epochs'])))\n",
    "    \n",
    "    # train\n",
    "    best_model = None\n",
    "    best_f1_eval = -1\n",
    "    best_test_results = None\n",
    "    best_val_results = None\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for each_epoch in range(args['epochs']):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            global_step += 1\n",
    "\n",
    "            # clean gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # unroll inputs and sent to device\n",
    "            input_batch = {\n",
    "                'input_ids': batch['input_ids'],\n",
    "                'attention_mask': batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            if global_step == 2:\n",
    "                print(input_batch['input_ids'].shape)\n",
    "\n",
    "            # forward pass\n",
    "            logits, logits_dec = model(input_batch)\n",
    "\n",
    "            # calculate loss\n",
    "            #loss_enc = loss_fct(\n",
    "            #    logits, \n",
    "            #    batch['encoder_labels'].to(device)\n",
    "            #)\n",
    "            loss_enc = 0\n",
    "            loss_dec = loss_fct_dec(\n",
    "                logits_dec.permute(0, 2, 1),\n",
    "                batch['decoder_labels'].to(device)\n",
    "            )            \n",
    "\n",
    "            # backpropagation\n",
    "            loss_avg = (args['wt_classification'] * loss_enc) + (args['wt_generation'] * loss_dec)\n",
    "            loss_avg.backward()\n",
    "\n",
    "            # update parameters and lr\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            # log info to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train/classification_loss\": loss,\n",
    "                    \"train/generation_loss\": loss_dec,\n",
    "                    \"train/weighted_loss\": loss_avg,\n",
    "                    \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"epoch\": each_epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "\n",
    "            if global_step%args['eval_every_steps'] == 0:\n",
    "                # evaluate model\n",
    "                val_predictions = evaluate(\n",
    "                    model=model, \n",
    "                    data_loader=val_loader,\n",
    "                    objective_f=loss_fct,\n",
    "                )\n",
    "                val_results = get_performance(\n",
    "                    actual_=val_predictions['actual'], \n",
    "                    preds_=val_predictions['preds'], \n",
    "                    dict_mapping=label2id\n",
    "                )\n",
    "\n",
    "                # log info to wandb\n",
    "                #eval_log = {'val/%s'%k: v['f1-score'] for k,v in metrics.items() if isinstance(v, dict)}\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"val/precision\": val_results['metrics']['macro avg']['precision'],\n",
    "                        \"val/recall\": val_results['metrics']['macro avg']['recall'],\n",
    "                        \"val/f1\": val_results['metrics']['macro avg']['f1-score'],\n",
    "                        \"val/accuracy\": val_results['metrics']['accuracy'],\n",
    "                        \"val/loss\": val_predictions['loss'],\n",
    "                        \"epoch\": each_epoch,\n",
    "\n",
    "                        \"val/precision_yes\": val_results['metrics']['0']['precision'],\n",
    "                        \"val/precision_no\": val_results['metrics']['1']['precision'],\n",
    "                        #\"val/precision_maybe\": val_results['metrics']['2']['precision'],\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "\n",
    "                # update best model\n",
    "                if best_f1_eval < val_results['metrics']['macro avg']['f1-score']:\n",
    "                    best_model = deepcopy(model).to(device)\n",
    "                    best_val_results = deepcopy(val_results)\n",
    "                    best_f1_eval = val_results['metrics']['macro avg']['f1-score']\n",
    "\n",
    "    \n",
    "    # test the model based on best_model\n",
    "    test_predictions = evaluate(\n",
    "        model=best_model, \n",
    "        data_loader=test_loader,\n",
    "        objective_f=loss_fct,\n",
    "    )\n",
    "    best_test_results = get_performance(\n",
    "        actual_=test_predictions['actual'], \n",
    "        preds_=test_predictions['preds'], \n",
    "        dict_mapping=label2id\n",
    "    )\n",
    "    \n",
    "    # save the results and the model\n",
    "    model_dict[model_idx]['results'] = {\n",
    "        'validation_results': deepcopy(best_val_results),\n",
    "        'test_results': deepcopy(best_test_results),\n",
    "        'trained_model': deepcopy(best_model),\n",
    "    }\n",
    "    \n",
    "    #\n",
    "    print('\\n')\n",
    "    print('='*5)\n",
    "    print('Results for model\\t : %s'%model_dict[model_idx]['model'])\n",
    "    print('='*5)\n",
    "    print('Precision \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['precision'])\n",
    "    print('Recall \\t\\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['recall'])\n",
    "    print('f1-score \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['f1-score'])\n",
    "    print('Accuracy \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['accuracy'])\n",
    "    print('='*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24727afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff68daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27e447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5036be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0ea8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdafb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a47ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da8fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad64f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618c29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59ebe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42d326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786379d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0d164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7704c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
