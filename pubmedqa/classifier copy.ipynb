{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88aece49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8235d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705aeaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/vijeta/miniconda3/envs/bioqa/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dff83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2dee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(split, fold=1):\n",
    "    if split == 'train':\n",
    "        train_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/train_set.json' % fold, \n",
    "                                    'r'))\n",
    "        dev_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/pqal_fold%d/dev_set.json' % fold, \n",
    "                                  'r'))\n",
    "        final_json = {**train_json, **dev_json}\n",
    "    else:\n",
    "        test_json = json.load(open('/mnt/nfs/work1/hongyu/brawat/pubmedqa/pubmedqa/data/test_set.json', 'r'))\n",
    "        final_json = test_json\n",
    "    list_data = []\n",
    "    for key_, val_ in final_json.items():\n",
    "        tmp_ = {'sentence1': val_['QUESTION'], \n",
    "                'sentence2': ' '.join(val_['CONTEXTS']), \n",
    "                'gold_label': val_['final_decision']}\n",
    "        list_data.append(tmp_)\n",
    "    return list_data\n",
    "\n",
    "def read_data_(dict_data_):\n",
    "    \n",
    "    list_data = []\n",
    "    for idx in range(len(dict_data_['question'])):\n",
    "        instance = {\n",
    "            'sentence1': dict_data_['question'][idx],\n",
    "            'sentence2': ''.join(dict_data_['context'][idx]['contexts']),\n",
    "            'gold_label': dict_data_['final_decision'][idx]\n",
    "        }\n",
    "        list_data.append(instance)\n",
    "    \n",
    "    return list_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629018c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_wts(dict_cnt, alpha=15):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = np.log(alpha * tot_cnt/dict_cnt[each_cat])\n",
    "    return wt_\n",
    "\n",
    "def get_class_dist(dict_cnt):\n",
    "    tot_cnt = sum([dict_cnt[x] for x in dict_cnt])\n",
    "    wt_ = {}\n",
    "    for each_cat in dict_cnt:\n",
    "        wt_[each_cat] = dict_cnt[each_cat]/tot_cnt\n",
    "    return wt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3588c44b-c86e-4d82-9cd7-9317a473b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/users/vijeta/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 208.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['pubid', 'question', 'context', 'long_answer', 'final_decision'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pubmedqa = datasets.load_dataset('pubmed_qa', 'pqa_labeled')\n",
    "pubmedqa_train, pubmedqa_test = train_test_split(pubmedqa['train'])\n",
    "\n",
    "pubmedqa_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f827a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_data = {}\n",
    "#dict_data['train'] = read_data(split='train', fold=1)\n",
    "#dict_data['test'] = read_data(split='test')\n",
    "dict_data['train'] = read_data_(pubmedqa_train)\n",
    "dict_data['test'] = read_data_(pubmedqa_test)\n",
    "\n",
    "label2id = {'yes':0, 'no': 1, 'maybe': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6815ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Assessing Patient Reported Outcomes Measures via Phone Interviews Versus Patient Self-Survey in the Clinic: Are We Measuring the Same Thing?',\n",
       " 'sentence2': 'Longitudinally following patients requires a full-time employee (FTE)-dependent data inflow infrastructure. There are efforts to capture patient-reported outcomes (PROs) by the use of non-FTE-dependent methodologies. In this study, we set out to assess the reliability of PRO data captured via FTE-dependent compared with non-FTE-dependent methodologies.A total of 119 adult patients (65 men) who underwent 1-and 2-level lumbar fusions at Duke University Medical Center were enrolled in this prospective study. Enrollment criteria included available demographic, clinical, and PRO data. All patients completed 2 sets of questionnaires--the first a phone interviews and the second a self-survey. There was at least a 2-week period between the phone interviews and self-survey. Questionnaires included the Oswestry Disability Index (ODI), the visual analog scale for back pain (VAS-BP), and the visual analog scale for leg pain (VAS-LP). Repeated-measures analysis of variance was used to compare the reliability of baseline PRO data captured.A total of 39.49% of patients were smokers, 21.00% had diabetes, and 11.76% had coronary artery disease; 26.89% reported history of anxiety disorder, and 28.57% reported history of depression. A total of 97.47% of patients had a high-school diploma or General Education Development, and 49.57% attained a 4-year college degree or postgraduate degree. We observed a high correlation between baseline PRO data captured between FTE-dependent versus non-FTE dependent methodologies (ODI: r = -0.89, VAS-BP: r = 0.74, VAS-LP: r = 0.70). There was no difference in PROs of baseline pain and functional disability between FTE-dependent and non-FTE-dependent methodologies: baseline ODI (FTE-dependent: 47.73 ± 16.77 [mean ± SD] vs. non-FTE-dependent: 45.81 ± 12.11, P = 0.39), VAS-LP (FTE-dependent: 6.13 ± 2.78 vs. non-FTE-dependent: 6.46 ± 2.79, P = 0.36) and VAS-BP (FTE-dependent: 6.33 ± 2.90 vs. non-FTE-dependent: 6.53 ± 2.48, P = 0.57).',\n",
       " 'gold_label': 'yes'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c290a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Train\n",
      "====================\n",
      "Train:  Counter({'yes': 410, 'no': 261, 'maybe': 79})\n",
      "Train:  94.73466666666667\n",
      "Train:  1346.324\n",
      "\n",
      "\n",
      "====================\n",
      "Test\n",
      "====================\n",
      "Test:  Counter({'yes': 142, 'no': 77, 'maybe': 31})\n",
      "Test:  92.58\n",
      "Test:  1316.652\n"
     ]
    }
   ],
   "source": [
    "print(\"==\"*10)\n",
    "print('Train')\n",
    "print(\"==\"*10)\n",
    "class_counts = Counter([x['gold_label'] for x in dict_data['train']])\n",
    "print(\"Train: \", Counter([x['gold_label'] for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence1'].__len__() for x in dict_data['train']]))\n",
    "print(\"Train: \", np.mean([x['sentence2'].__len__() for x in dict_data['train']]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"==\"*10)\n",
    "print(\"Test\")\n",
    "print(\"==\"*10)\n",
    "print(\"Test: \", Counter([x['gold_label'] for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence1'].__len__() for x in dict_data['test']]))\n",
    "print(\"Test: \", np.mean([x['sentence2'].__len__() for x in dict_data['test']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8aef64-9d0c-4553-9c54-6834a7df1d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b3c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 1.7025283355001124, 'no': 2.1541650878757723, 'maybe': 3.349237642731444}\n",
      "{'yes': 0.5466666666666666, 'no': 0.348, 'maybe': 0.10533333333333333}\n"
     ]
    }
   ],
   "source": [
    "#class_wts = get_class_wts(dict_cnt={'yes': 276, 'no': 169, 'maybe': 55}, \n",
    "#                          alpha=3)\n",
    "\n",
    "class_wts = get_class_wts(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }, \n",
    "    alpha=3\n",
    ")\n",
    "print(class_wts)\n",
    "\n",
    "class_dist = get_class_dist(\n",
    "    dict_cnt={\n",
    "        'yes': class_counts['yes'], \n",
    "        'no': class_counts['no'], \n",
    "        'maybe': class_counts['maybe'],\n",
    "    }\n",
    ")\n",
    "print(class_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4cda53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            finetuning_task='pubmedqa'\n",
    "        )\n",
    "        self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(**batch_)\n",
    "        #pooled = torch.mean(outputs[0], dim=1).to(device)\n",
    "        #logits_ = self.classifier(pooled)\n",
    "        logits_ = outputs[0]\n",
    "        \n",
    "        return logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17afa8a6-08e4-4460-be7c-7d1f0922ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilliary functions\n",
    "\n",
    "def inspect_dataloader(loaders):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    #\n",
    "    print(f\"\\nSize of the training set is {len(loaders.dataset_train)}\")\n",
    "    print(f\"Size of the validation set is {len(loaders.dataset_validation)}\")\n",
    "    print(f\"Size of the test set is {len(loaders.dataset_test)}\")\n",
    "    \n",
    "    #\n",
    "    check_first = loaders.dataset_validation[0]['input_ids'] == loaders.dataset_test[0]['input_ids']\n",
    "    check_last = loaders.dataset_validation[-1]['input_ids'] == loaders.dataset_test[-1]['input_ids']\n",
    "    print(f\"\\nFirst example in test and validation set is same: {check_first}\")\n",
    "    print(f\"Last example in test and validation set is same: {check_last}\")\n",
    "    \n",
    "    \n",
    "    #\n",
    "    print(\"\\nPrinting three randomly sampled examples...\")\n",
    "    random_samples = np.random.randint(0, len(loaders.dataset_train), size=3)\n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = loaders.dataset_train[sample_]\n",
    "        tokenizer = loaders.source_tokenizer\n",
    "        id2label = loaders.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('Gold label is as follows:')\n",
    "        print(id2label[tokenized_sample['gold_label'][0]])        \n",
    "    \n",
    "    return\n",
    "\n",
    "def get_grouped_parameters(\n",
    "    model_in, \n",
    "    no_decay_layers, \n",
    "    weight_decay\n",
    "):\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model_in.named_parameters() if not any(nd in n for nd in no_decay_layers)],\n",
    "         'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model_in.named_parameters() if any(nd in n for nd in no_decay_layers)], \n",
    "         'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "def evaluate(model, data_loader, objective_f):\n",
    "    model.eval()\n",
    "    dict_result = {'actual':[],\n",
    "                   'preds':[]}\n",
    "    \n",
    "    #print('\\nStarting model evaluation:')\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            \n",
    "            \n",
    "            # unroll features\n",
    "            dict_result['actual'] += batch['encoder_labels'].numpy().tolist()\n",
    "            input_batch = {\n",
    "                'input_ids':batch['input_ids'],\n",
    "                'attention_mask':batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "            \n",
    "            # calculate loss\n",
    "            eval_loss += objective_f(logits, batch['encoder_labels'].view(-1).to(device)).item()\n",
    "            \n",
    "            # update\n",
    "            dict_result['preds'] += np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "    \n",
    "    # update\n",
    "    dict_result['actual'] = [x for x in dict_result['actual']]\n",
    "    dict_result['loss'] = eval_loss / (batch_idx + 1)\n",
    "    \n",
    "    return dict_result\n",
    "\n",
    "def get_performance(\n",
    "    actual_, \n",
    "    preds_,\n",
    "    dict_mapping\n",
    "):\n",
    "    results = {}\n",
    "    \n",
    "    # accuracy, precision, recall, f1\n",
    "    results['metrics'] = classification_report(\n",
    "        actual_, \n",
    "        preds_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    for name_, cls_ in dict_mapping.items():\n",
    "        if not str(cls_) in results['metrics']:\n",
    "            results['metrics'][str(cls_)] = {'precision': 0}\n",
    "            print(f\"\\nUnique gold labels in the current batch are: {list(set(actual_))}\")\n",
    "            print(f\"Unique predicted labels are: {list(set(preds_))}\")\n",
    "    \n",
    "    # confusion matrix\n",
    "    results['confusion_matrix'] = pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            actual_, \n",
    "            preds_\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # counter\n",
    "    results['actual_counter'] = Counter(actual_)\n",
    "    results['prediction_counter'] = Counter(preds_)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaab0f-98e9-4536-b861-a91d27f73841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec39a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'roberta-base'\n",
    "#tokenizer_name = 'roberta-base'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 10,\n",
    "    'learning_rate': 6.2e-6,\n",
    "    'epochs': 100,\n",
    "    'eval_every_steps': 300,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 8,\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c052db2e-c4f9-45ca-96aa-302b9a4f987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from PubMedQAData_EncDec import QADataLoader\n",
    "label2id = {'yes': 0, 'no': 1, 'maybe': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c68844-0ef8-4f4f-b6ca-f51384ec9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'allenai/biomed_roberta_base',\n",
    "        'tokenizer': 'allenai/biomed_roberta_base',\n",
    "    },\n",
    "    1: {\n",
    "        'model': 'dmis-lab/biobert-v1.1',\n",
    "        'tokenizer': 'dmis-lab/biobert-v1.1',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b51771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training of model: allenai/biomed_roberta_base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvijetakd\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nonraidv/home/users/vijeta/QA_bechmarking/bioQA/pubmedqa/wandb/run-20220520_224536-2q6wh5s1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA/runs/2q6wh5s1\" target=\"_blank\">skilled-yogurt-119</a></strong> to <a href=\"https://wandb.ai/vijetakd/Bio-Med-Clinical%20QA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pubmed_qa (/home/users/vijeta/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 351.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 463.68it/s]\n",
      "Reusing dataset pubmed_qa (/home/users/vijeta/.cache/huggingface/datasets/pubmed_qa/pqa_unlabeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 413.44it/s]\n",
      "  0%|▍                                                                                                         | 272/61249 [00:37<2:19:14,  7.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2059090/56868802.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_sequence_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#inspect_dataloader(dataloaders)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nonraidv/home/users/vijeta/QA_bechmarking/bioQA/pubmedqa/PubMedQAData_EncDec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datasets_name, datasets_config, filepath_data, label2id, filepath_label2id, tokenizer_name, max_sequence_length, batch_size, debug, debug_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdata_unl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pqa_unlabeled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_unl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_list_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_unl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nonraidv/home/users/vijeta/QA_bechmarking/bioQA/pubmedqa/PubMedQAData_EncDec.py\u001b[0m in \u001b[0;36mget_list_data\u001b[0;34m(self, dict_data_)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;34m'source_question'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict_data_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m#'source_context': ''.join(dict_data_['context'][idx]['contexts']),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;34m'target_answer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict_data_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'long_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;34m'gold_label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_decision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             }\n",
      "\u001b[0;32m~/miniconda3/envs/bioqa/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2123\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m         return self._getitem(\n\u001b[0;32m-> 2125\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m         )\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bioqa/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m         formatted_output = format_table(\n\u001b[0;32m-> 2110\u001b[0;31m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2111\u001b[0m         )\n\u001b[1;32m   2112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bioqa/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bioqa/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bioqa/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bioqa/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pylist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_idx in model_dict:\n",
    "    print('\\nStarting training of model: %s'%(model_dict[model_idx]['model']))\n",
    "    \n",
    "    #\n",
    "    model_name = model_dict[model_idx]['model'].split('/')[-1]\n",
    "    args['output_dir'] = model_name\n",
    "    if not os.path.exists(args['output_dir']):\n",
    "        os.mkdir(args['output_dir'])\n",
    "    \n",
    "    #\n",
    "    args['model'] = model_dict[model_idx]['model']\n",
    "    wandb.init(\n",
    "        project='Bio-Med-Clinical QA', \n",
    "        config=args\n",
    "    )\n",
    "    \n",
    "    # get dataloaders for training and testing\n",
    "    dataloaders = QADataLoader(\n",
    "        datasets_name='pubmed_qa',\n",
    "        datasets_config='pqa_labeled',\n",
    "        label2id=label2id,\n",
    "        tokenizer_name=model_dict[model_idx]['tokenizer'],\n",
    "        max_sequence_length=args['max_sequence_length'],\n",
    "        batch_size=args['batch_size'],\n",
    "        debug=False\n",
    "    )\n",
    "    #inspect_dataloader(dataloaders)\n",
    "    break\n",
    "\n",
    "    #\n",
    "    train_loader = dataloaders.dataloader_train\n",
    "    val_loader = dataloaders.dataloader_validation\n",
    "    test_loader = dataloaders.dataloader_test\n",
    "    \n",
    "    # set total steps and warmp-up steps for sheduler\n",
    "    args['t_total'] = len(train_loader) // args['gradient_accumulation_steps'] * args['epochs']\n",
    "    args['warmup_steps'] = int(0.20*args['t_total'])\n",
    "\n",
    "    # define model\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dict[model_idx]['model'], \n",
    "        config=config,\n",
    "    )\n",
    "    \"\"\"\n",
    "    #\n",
    "    model = QAModel(\n",
    "        model_name=model_dict[model_idx]['model'],\n",
    "        num_classes=dataloaders.num_classes,\n",
    "    )\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier.weight' in name:\n",
    "            torch.nn.init.zeros_(param.data)\n",
    "        elif 'classifier.bias' in name:\n",
    "            param.data = torch.tensor([class_dist['yes'], class_dist['no'], class_dist['maybe']]).float()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        get_grouped_parameters(model, no_decay, args['weight_decay']), \n",
    "        lr=args['learning_rate'], \n",
    "        eps=args['adam_epsilon']\n",
    "    )\n",
    "\n",
    "    # scheduler for lr\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=args['warmup_steps'],\n",
    "        num_training_steps=args['t_total']\n",
    "    )\n",
    "\n",
    "    # objective function\n",
    "    loss_fct = CrossEntropyLoss(\n",
    "        weight=torch.tensor(list(class_wts.values())).float().to(device), \n",
    "        ignore_index=-100,\n",
    "    )\n",
    "    \n",
    "    # train\n",
    "    best_model = None\n",
    "    best_f1_eval = -1\n",
    "    best_test_results = None\n",
    "    best_val_results = None\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for each_epoch in tqdm(range(args['epochs'])):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            # clean gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # unroll inputs and sent to device\n",
    "            input_batch = {\n",
    "                'input_ids': batch['input_ids'],\n",
    "                'attention_mask': batch['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "\n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fct(logits, batch['encoder_labels'].to(device))\n",
    "            \n",
    "            # log info to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train/loss\": loss,\n",
    "                    \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                    \"epoch\": each_epoch,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "            \n",
    "\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters and lr\n",
    "            optimizer.step()\n",
    "            scheduler.step()  \n",
    "\n",
    "            if global_step%args['eval_every_steps'] == 0:\n",
    "                # evaluate model\n",
    "                val_predictions = evaluate(\n",
    "                    model=model, \n",
    "                    data_loader=val_loader,\n",
    "                    objective_f=loss_fct,\n",
    "                )\n",
    "                val_results = get_performance(\n",
    "                    actual_=val_predictions['actual'], \n",
    "                    preds_=val_predictions['preds'], \n",
    "                    dict_mapping=label2id\n",
    "                )\n",
    "\n",
    "                # log info to wandb\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"eval/precision\": val_results['metrics']['macro avg']['precision'],\n",
    "                        \"eval/recall\": val_results['metrics']['macro avg']['recall'],\n",
    "                        \"eval/f1\": val_results['metrics']['macro avg']['f1-score'],\n",
    "                        \"eval/accuracy\": val_results['metrics']['accuracy'],\n",
    "                        \"eval/loss\": val_predictions['loss'],\n",
    "                        \"epoch\": each_epoch,\n",
    "\n",
    "                        \"eval/precision_yes\": val_results['metrics']['0']['precision'],\n",
    "                        \"eval/precision_no\": val_results['metrics']['1']['precision'],\n",
    "                        \"eval/precision_maybe\": val_results['metrics']['2']['precision'],\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "\n",
    "                # update best model\n",
    "                if best_f1_eval < val_results['metrics']['weighted avg']['f1-score']:\n",
    "                    #best_model = deepcopy(model).to(device)\n",
    "                    best_val_results = deepcopy(val_results)\n",
    "                    best_f1_eval = val_results['metrics']['weighted avg']['f1-score']\n",
    "                    \n",
    "                    # save model\n",
    "                    torch.save(model.state_dict(), os.path.join(args['output_dir'],  model_name+'.pt'))\n",
    "                    \n",
    "\n",
    "    \n",
    "    # test the model based on best_model\n",
    "    model.load_state_dict(torch.load(os.path.join(args['output_dir'],  model_name+'.pt')))\n",
    "    test_predictions = evaluate(\n",
    "        model=model, \n",
    "        data_loader=test_loader,\n",
    "        objective_f=loss_fct,\n",
    "    )\n",
    "    best_test_results = get_performance(\n",
    "        actual_=test_predictions['actual'], \n",
    "        preds_=test_predictions['preds'], \n",
    "        dict_mapping=label2id\n",
    "    )\n",
    "    \n",
    "    # save the results and the model\n",
    "    model_dict[model_idx]['results'] = {\n",
    "        'validation_results': deepcopy(best_val_results),\n",
    "        'test_results': deepcopy(best_test_results),\n",
    "        #'trained_model': deepcopy(best_model),\n",
    "    }\n",
    "    \n",
    "    #\n",
    "    print('\\n')\n",
    "    print('='*5)\n",
    "    print('Results for model\\t : %s'%model_dict[model_idx]['model'])\n",
    "    print('='*5)\n",
    "    print('Precision \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['precision'])\n",
    "    print('Recall \\t\\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['recall'])\n",
    "    print('f1-score \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['macro avg']['f1-score'])\n",
    "    print('Accuracy \\t\\t = %f'%model_dict[model_idx]['results']['test_results']['metrics']['accuracy'])\n",
    "    print('='*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_dict[model_idx]['model'].split('/')[-1]\n",
    "torch.save(model.state_dict(), os.path.join(args['output_dir'],  model_name+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24727afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(args['output_dir'],  model_name+'.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff68daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27e447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5036be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0ea8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdafb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a47ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da8fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad64f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618c29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59ebe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42d326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1786379d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0d164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7704c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
