{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4b23a6-a674-458b-84da-bc2456887356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/vijeta/miniconda3/envs/bioqa/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "from data_pub import pubmedDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (BertPreTrainedModel, BertModel, AdamW, get_linear_schedule_with_warmup, \n",
    "                          RobertaPreTrainedModel, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel, AutoConfig)\n",
    "from transformers import (WEIGHTS_NAME,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                          XLMConfig, XLMForSequenceClassification, XLMTokenizer,\n",
    "                          DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "from PubMedQAData_EncDec import QADataLoader\n",
    "import wandb\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddebc702-70e9-4e82-a31a-b32030429e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "    ):\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            finetuning_task='pubmedqa'\n",
    "        )\n",
    "        self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            in_features=768,\n",
    "            out_features=num_classes,\n",
    "        )\n",
    "    \n",
    "        return\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_,\n",
    "    ):\n",
    "        outputs = self.encoder(**batch_)\n",
    "        #pooled = torch.mean(outputs[0], dim=1).to(device)\n",
    "        #logits_ = self.classifier(pooled)\n",
    "        logits_ = outputs[0]\n",
    "        \n",
    "        return logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0cca134-3065-4661-8c81-ed66798a8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for collecting all predictions on the input dataset\n",
    "def get_predictions(model_, loader_):\n",
    "    model_.eval()\n",
    "    \n",
    "    #\n",
    "    dict_results = {\n",
    "        'encoder_labels_artificial': [],\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'decoder_input_ids': [],\n",
    "        'decoder_attention_mask': [],\n",
    "        'decoder_labels': [],\n",
    "    }\n",
    "    for batch_idx, batch_ in tqdm(enumerate(loader_)):\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            # unroll features\n",
    "            input_batch = {\n",
    "                'input_ids':batch_['input_ids'],\n",
    "                'attention_mask':batch_['attention_mask']\n",
    "            }\n",
    "            input_batch = {k: v.to(device) for k, v in input_batch.items()}\n",
    "            \n",
    "            # forward pass\n",
    "            logits = model(input_batch)\n",
    "            \n",
    "            # update\n",
    "            dict_results['encoder_labels_artificial'] += np.argmax(logits.detach().cpu().numpy(), axis=1).tolist()\n",
    "            for k_ in batch_:\n",
    "                if not k_ == 'encoder_labels':\n",
    "                    dict_results[k_] += batch_[k_].numpy().tolist()\n",
    "    \n",
    "    return dict_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f16cba2-9559-4b62-b67f-d50976bd1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we get the data with artificial label we will need to convert it back to the required format, following class does that\n",
    "\n",
    "class CustomArtiDataloader():\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dict_data: dict,\n",
    "        label2id: dict,\n",
    "        batch_size: int = 16,\n",
    "        debug: bool = False,\n",
    "        debug_size: int = 8,\n",
    "    ):\n",
    "        data = self.to_list(dict_data)\n",
    "        \n",
    "        # define Dataset object\n",
    "        self.dataset = CustomArtiDataset(data)\n",
    "        \n",
    "        # define dataloader object\n",
    "        self.dataloader = Dataloader(\n",
    "            self.dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collation_f,            \n",
    "        )\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def to_list(self, data_in):\n",
    "        \n",
    "        data_out = []\n",
    "        for idx_ in range(len(data_in['input_ids'])):\n",
    "            instance = {k_: v_[idx_] for k_, v_ in data_in.items()}\n",
    "            data_out.append(instance)\n",
    "            \n",
    "        return data_out\n",
    "    \n",
    "    def collation_f(self, batch):\n",
    "        \n",
    "        #\n",
    "        input_ids_list = [ex[\"input_ids\"] for ex in batch]\n",
    "        attention_mask_list = [ex[\"attention_mask\"] for ex in batch]\n",
    "        decoder_input_ids_list = [ex[\"decoder_input_ids\"] for ex in batch]\n",
    "        decoder_attention_mask_list = [ex[\"decoder_attention_mask\"] for ex in batch]\n",
    "        decoder_labels_list = [ex[\"decoder_labels\"] for ex in batch]\n",
    "        encoder_label_list = [ex['encoder_labels_artificial'] for ex in batch]\n",
    "\n",
    "        collated_batch = {\n",
    "            \"input_ids\": torch.LongTensor(input_ids_list),\n",
    "            \"attention_mask\": torch.LongTensor(attention_mask_list),\n",
    "            \"encoder_labels\": torch.LongTensor(encoder_label_list),\n",
    "            \"decoder_input_ids\": torch.LongTensor(decoder_input_ids_list),\n",
    "            \"decoder_attention_mask\": torch.LongTensor(decoder_attention_mask_list),\n",
    "            \"decoder_labels\": torch.LongTensor(decoder_labels_list),\n",
    "        }\n",
    "\n",
    "        return collated_batch\n",
    "    \n",
    "class CustomArtiDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, list_data):\n",
    "        self.data = list_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return list_data[idx]\n",
    "\n",
    "#\n",
    "def inspect_dataloader(loaders):\n",
    "    print('Inspecting dataloader...')\n",
    "    \n",
    "    #\n",
    "    print(f\"\\nSize of the training set is {len(loaders.dataset_train)}\")\n",
    "    print(f\"Size of the validation set is {len(loaders.dataset_validation)}\")\n",
    "    print(f\"Size of the test set is {len(loaders.dataset_test)}\")\n",
    "    \n",
    "    #\n",
    "    check_first = loaders.dataset_validation[0]['input_ids'] == loaders.dataset_test[0]['input_ids']\n",
    "    check_last = loaders.dataset_validation[-1]['input_ids'] == loaders.dataset_test[-1]['input_ids']\n",
    "    print(f\"\\nFirst example in test and validation set is same: {check_first}\")\n",
    "    print(f\"Last example in test and validation set is same: {check_last}\")\n",
    "    \n",
    "    \n",
    "    #\n",
    "    print(\"\\nPrinting three randomly sampled examples...\")\n",
    "    random_samples = np.random.randint(0, len(loaders.dataset_train), size=3)\n",
    "    for sample_ in random_samples:\n",
    "        tokenized_sample = loaders.dataset_train[sample_]\n",
    "        tokenizer = loaders.source_tokenizer\n",
    "        id2label = loaders.id2label\n",
    "        \n",
    "        #\n",
    "        print('\\nInput sequence to the model i.e. Question + Context, is as follows:')\n",
    "        print(tokenizer.decode(tokenized_sample['input_ids']))\n",
    "        print('Gold label is as follows:')\n",
    "        print(id2label[tokenized_sample['gold_label'][0]])        \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d6789a-c637-4347-bddf-0a8ed73a8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2:\n",
    "# Step 1: get dataloader for unlabled and artifial dataset\n",
    "# Step 2: instantiate biomed-roberta model and load previously trained model\n",
    "# Step 3: use loaded model to predict artificial labels\n",
    "# Step 4: convert the predictions into dataloader\n",
    "# Step 5: train BioMedRoberta on artificial data\n",
    "# Step 6: save the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02a7cd01-f0ee-4812-951a-ce18dc4d04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = {\n",
    "    'weight_decay': 10,\n",
    "    'learning_rate': 6.2e-6,\n",
    "    'epochs': 100,\n",
    "    'eval_every_steps': 300,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_sequence_length': 512,\n",
    "    'batch_size': 768,\n",
    "    'output_dir': r'./biomed_roberta_base_best',\n",
    "}\n",
    "label2id = {\n",
    "    'yes': 0,\n",
    "    'no': 1,\n",
    "    'maybe': 2,\n",
    "}\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "#\n",
    "model_dict = {\n",
    "    0: {\n",
    "        'model': 'allenai/biomed_roberta_base',\n",
    "        'tokenizer': 'allenai/biomed_roberta_base',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7983248f-7bd3-4629-885a-527bd6025e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211269it [00:00, 343058.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Dataloader\n",
    "\n",
    "#\n",
    "data_all = QADataLoader(\n",
    "    datasets_name=None,#'pubmed_qa',\n",
    "    datasets_config=None,#'pqa_unlabeled',\n",
    "    label2id=label2id,\n",
    "    tokenizer_name=model_dict[0]['tokenizer'],\n",
    "    max_sequence_length=args['max_sequence_length'],\n",
    "    batch_size=args['batch_size'],\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89b978e-3176-4d88-bce4-bba1224d667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting dataloader...\n",
      "\n",
      "Size of the training set is 209156\n",
      "Size of the validation set is 1057\n",
      "Size of the test set is 1056\n",
      "\n",
      "First example in test and validation set is same: False\n",
      "Last example in test and validation set is same: False\n",
      "\n",
      "Printing three randomly sampled examples...\n",
      "\n",
      "Input sequence to the model i.e. Question + Context, is as follows:\n",
      "<s>Does addition of clonidine in caudal anesthesia in children increase duration of post-operative analgesia?</s></s>Pain in infancy is a developmental process. Due to the underdeveloped pain pathways in the spinal cord, the threshold of stimulation and sensation of pain is low at birth and has potential impacts on increasing the central effects of pain. Primary trauma during infancy can cause long term changes in structure and function of pain pathways that continue until adulthood. Lack of pain management in children can result in morbidity and mortality. In this study we examined the duration of post-operative analgesia in children when clonidine is added to bupivacaine in caudal anesthesia. In this clinical trial, 40 children aged 1-8 years who were candidates for elective inguinal hernia repair were studied. Induction and maintenance of anesthesia were achieved using sodium thiopenthal, halothane and nitrous oxide. Children were randomly divided into 2 groups in a double-blind fashion, and were given caudal anesthesia with 0.125% bupivacaine (1ml/kg) alone or b bupivacaine plus 2 Î¼g/kg clonidine. Blood pressure and heart rate were recorded peri-operatively. Analgesia was evaluated using objective pain scale (OPS) and sedation was assessed using Ramsay sedation scale (RSS). Acetaminophen was administered rectally for cases with OPS score greater than five. Duration of analgesia was found to be significantly longer in the group given bupivacaine plus clonidine (mean 417.50 min vs. 162.00 min). Peri-operative hypotension or bradycardia, post-operative respiratory depression, nausea or vomiting were not recorded in any patient.</s>\n",
      "Gold label is as follows:\n",
      "yes\n",
      "\n",
      "Input sequence to the model i.e. Question + Context, is as follows:\n",
      "<s>Does increased gut permeability early after burns correlate with the extent of burn injury?</s></s>To determine if increased gut permeability within 48 hrs after burn injury correlates with the extent of injury, before sepsis and pulmonary disorders have complicated the clinical course. Nonrandomized, controlled study. Consecutive patients admitted with burn injuries on > 20% of body surface area. Intestinal absorption and renal excretion of polyethylene glycol 3350 was used as the macromolecule to determine gut permeability; polyethylene glycol 400 intestinal absorption was used as an internal control for abnormal motility and malabsorption. Polyethylene glycol 3350 (40 g) and polyethylene glycol 400 (5 g) were administered enterally. Gut permeability was significantly increased early after the injury. The patients excreted 0.56 +/- 0.34% (n = 11) of polyethylene glycol 3350, compared with the amount (0.12 +/- 0.04%) (p <.05) previously reported in normal volunteers. There was no significant difference in the excretion of polyethylene glycol 400 in the patients (27.0 +/- 4.6%, n = 11) vs. the normal volunteers previously reported (26.3 +/- 5.1%, n = 12), suggesting normal intestinal motility and absorption. The percentage of excretion of polyethylene glycol 3350 correlated with the percentage body surface burned; patients with smaller injuries excreted 0.32 +/- 0.17% (n = 6), which was greater than normal and less than those values from patients with larger injuries, 0.84 +/- 0.25% (n = 5) (p <.001 by Turkey test).</s>\n",
      "Gold label is as follows:\n",
      "yes\n",
      "\n",
      "Input sequence to the model i.e. Question + Context, is as follows:\n",
      "<s>Is adjuvant chemotherapy associated with decreased mortality after radical cystectomy for locally advanced bladder cancer?</s></s>We sought to evaluate the association of adjuvant chemotherapy with the risk of subsequent mortality among patients with locally advanced urothelial carcinoma (UC) of the bladder undergoing radical cystectomy (RC). We identified 675 patients who underwent RC for pT2-4 and/or N+ UC between 1980 and 2005. Adjuvant chemotherapy was defined as treatment within 90 days of RC. Survival was estimated using the Kaplan-Meier method and compared according to receipt of adjuvant chemotherapy with the log-rank test. Multivariate models were used to analyze the impact of adjuvant chemotherapy on disease progression and survival. A total of 80 (12 %) patients received adjuvant chemotherapy. Median age was 69 years [interquartile range (IQR) 63, 76]. Median follow-up was 11 years (IQR 8, 16). Patients receiving adjuvant chemotherapy were more likely to have pT3-4 tumors (71 vs. 61 %; p < 0.001) and pN+ (85 vs. 19 %; p < 0.001). The 5-year cancer-specific survival was 46 % in those receiving adjuvant chemotherapy and 51 % in those that did not (p = 0.63). The 5-year overall survival was 39 % in those receiving adjuvant chemotherapy and 38 % in those that did not (p = 0.24). When controlling for age, sex, stage, and performance status, adjuvant chemotherapy was associated with a 29 % decrease in the risk of bladder cancer death (HR 0.71, p = 0.06) and a 39 % decrease in the risk of all-cause mortality (HR 0.61, p = 0.002).</s>\n",
      "Gold label is as follows:\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "inspect_dataloader(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327259f4-6fef-4b95-933d-755bd165ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/biomed_roberta_base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at allenai/biomed_roberta_base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QAModel(\n",
       "  (encoder): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Model\n",
    "\n",
    "#\n",
    "model_name = model_dict[0]['model'].split('/')[-1]\n",
    "model = QAModel(\n",
    "    model_name=model_dict[0]['model'],\n",
    "    num_classes=data_all.num_classes,\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(args['output_dir'],  model_name+'.pt')))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a18ad78-a56d-4e28-ae92-363539823d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'encoder_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2105011/3869177295.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 3: Predict (get artificial labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2105011/3018555937.py\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(model_, loader_)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdict_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_labels_artificial'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mdict_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'encoder_labels'"
     ]
    }
   ],
   "source": [
    "# Step 3: Predict (get artificial labels)\n",
    "predictions = get_predictions(model, data_all.dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5603c5d-60c1-4005-bbac-95f6961da00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209156\n",
      "260\n",
      "260\n",
      "260\n",
      "260\n",
      "260\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "for k_ in predictions:\n",
    "    print(len(predictions[k_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8ac8d-24a7-4134-84d9-78d636f4254a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
